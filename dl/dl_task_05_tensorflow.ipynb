{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Bc4c3dFKC5NQ1NV5We0fHivy3FHoFJ5K","timestamp":1716275564353}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Narangaraw411/Dive-into-code/blob/main/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"aIQSz4uOAsik","executionInfo":{"status":"ok","timestamp":1716275832419,"user_tz":-480,"elapsed":12404,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow.compat.v1 as tf\n","tf.compat.v1.disable_eager_execution()\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.datasets import mnist\n","from matplotlib import pyplot as plt"]},{"cell_type":"markdown","source":["## [Problem 1] Looking back on the scratch, Preparing dataset"],"metadata":{"id":"MvsOP7TS5bEz"}},{"cell_type":"code","source":["df = pd.read_csv(\"Iris.csv\")\n","df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n","y = df[\"Species\"]\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","X = np.array(X)\n","y = np.array(y)\n","y[y == \"Iris-versicolor\"] = 0\n","y[y == \"Iris-virginica\"] = 1\n","y = y.astype(np.int64)[:, np.newaxis]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","y_train_one_hot = enc.fit_transform(y_train)\n","y_val_one_hot = enc.transform(y_val)\n","y_test_one_hot = enc.transform(y_test)\n","mmsc = MinMaxScaler()\n","X_train = mmsc.fit_transform(X_train)\n","X_test = mmsc.transform(X_test)\n","X_val = mmsc.transform(X_val)\n","# print(y_train_one_hot)"],"metadata":{"id":"wWN1b4k-6P94","executionInfo":{"status":"ok","timestamp":1716275841533,"user_tz":-480,"elapsed":563,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## [Problem 2] Consider the correspondence between Scratch and TensorFlow"],"metadata":{"id":"5AlrX-cw7JMR"}},{"cell_type":"code","source":["class SampleIterator():\n","    def __init__(self):\n","        self.X = [1,2,3,4,5]\n","        self.counter = 0\n","        self.stop = len(self.X)\n","    def __iter__(self):\n","        return self\n","    def __next__(self):\n","        if self.counter >= self.stop:\n","            raise StopIteration()\n","        x = self.X[self.counter]\n","        self.counter += 1\n","        return x\n","sample_iter = SampleIterator()\n","for x in sample_iter:\n","    print(x)\n","class GetMiniBatch:\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.intc)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]\n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","\n","#Hyperparameters\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 100\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 2\n","\n","#determine the shape of the network inputs and outputs\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","\n","#Mini-batch generation iterator\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9KoiO4j6wew","outputId":"1fcee936-e91d-41a5-e1f5-d7e8c5b36358","executionInfo":{"status":"ok","timestamp":1716275857924,"user_tz":-480,"elapsed":350,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","2\n","3\n","4\n","5\n"]}]},{"cell_type":"code","source":["def example_net(x):\n","    tf.random.set_random_seed(0)\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n","    return layer_output\n","\n","logits = example_net(X)\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","        total_loss /= n_samples\n","        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n","    print(\"test_acc : {:.3f}\".format(test_acc))\n","\n","with tf.Session()  as sess:\n","    sess.run(init)\n","    for epoch in  range(num_epochs):\n","        for i,  (mini_batch_x, mini_batch_y)  in enumerate(get_mini_batch_train):\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQzIqJVT-uIy","outputId":"f3742e45-d0d8-4c8f-cc5b-df82f28c5b2c","executionInfo":{"status":"ok","timestamp":1716275873801,"user_tz":-480,"elapsed":2262,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, loss : 1.8266, val_loss : 13.1328, acc : 0.438\n","Epoch 1, loss : 1.4494, val_loss : 10.6446, acc : 0.406\n","Epoch 2, loss : 1.0839, val_loss : 8.2323, acc : 0.438\n","Epoch 3, loss : 0.7486, val_loss : 6.1722, acc : 0.500\n","Epoch 4, loss : 0.5610, val_loss : 5.9173, acc : 0.500\n","Epoch 5, loss : 0.5283, val_loss : 5.7653, acc : 0.406\n","Epoch 6, loss : 0.4851, val_loss : 5.1725, acc : 0.469\n","Epoch 7, loss : 0.4305, val_loss : 4.5251, acc : 0.594\n","Epoch 8, loss : 0.3835, val_loss : 3.9915, acc : 0.625\n","Epoch 9, loss : 0.3434, val_loss : 3.5121, acc : 0.625\n","Epoch 10, loss : 0.3054, val_loss : 3.0665, acc : 0.625\n","Epoch 11, loss : 0.2677, val_loss : 2.6542, acc : 0.656\n","Epoch 12, loss : 0.2306, val_loss : 2.2668, acc : 0.656\n","Epoch 13, loss : 0.1948, val_loss : 1.9063, acc : 0.656\n","Epoch 14, loss : 0.1640, val_loss : 1.5705, acc : 0.688\n","Epoch 15, loss : 0.1383, val_loss : 1.2758, acc : 0.750\n","Epoch 16, loss : 0.1160, val_loss : 1.0246, acc : 0.781\n","Epoch 17, loss : 0.0978, val_loss : 0.8237, acc : 0.875\n","Epoch 18, loss : 0.0837, val_loss : 0.6640, acc : 0.906\n","Epoch 19, loss : 0.0729, val_loss : 0.5423, acc : 0.906\n","Epoch 20, loss : 0.0648, val_loss : 0.4553, acc : 0.906\n","Epoch 21, loss : 0.0586, val_loss : 0.3943, acc : 0.938\n","Epoch 22, loss : 0.0538, val_loss : 0.3494, acc : 0.969\n","Epoch 23, loss : 0.0499, val_loss : 0.3136, acc : 0.969\n","Epoch 24, loss : 0.0467, val_loss : 0.2840, acc : 0.969\n","Epoch 25, loss : 0.0438, val_loss : 0.2588, acc : 0.969\n","Epoch 26, loss : 0.0411, val_loss : 0.2373, acc : 0.969\n","Epoch 27, loss : 0.0387, val_loss : 0.2180, acc : 0.969\n","Epoch 28, loss : 0.0365, val_loss : 0.2003, acc : 0.969\n","Epoch 29, loss : 0.0345, val_loss : 0.1844, acc : 0.969\n","Epoch 30, loss : 0.0326, val_loss : 0.1702, acc : 0.969\n","Epoch 31, loss : 0.0310, val_loss : 0.1577, acc : 0.969\n","Epoch 32, loss : 0.0295, val_loss : 0.1468, acc : 0.969\n","Epoch 33, loss : 0.0283, val_loss : 0.1375, acc : 0.969\n","Epoch 34, loss : 0.0272, val_loss : 0.1293, acc : 0.969\n","Epoch 35, loss : 0.0262, val_loss : 0.1225, acc : 0.969\n","Epoch 36, loss : 0.0253, val_loss : 0.1171, acc : 0.969\n","Epoch 37, loss : 0.0245, val_loss : 0.1127, acc : 0.969\n","Epoch 38, loss : 0.0237, val_loss : 0.1090, acc : 0.969\n","Epoch 39, loss : 0.0230, val_loss : 0.1059, acc : 0.969\n","Epoch 40, loss : 0.0223, val_loss : 0.1031, acc : 0.969\n","Epoch 41, loss : 0.0216, val_loss : 0.1003, acc : 0.969\n","Epoch 42, loss : 0.0210, val_loss : 0.0974, acc : 0.969\n","Epoch 43, loss : 0.0204, val_loss : 0.0947, acc : 0.969\n","Epoch 44, loss : 0.0198, val_loss : 0.0924, acc : 0.969\n","Epoch 45, loss : 0.0192, val_loss : 0.0903, acc : 0.969\n","Epoch 46, loss : 0.0187, val_loss : 0.0882, acc : 0.969\n","Epoch 47, loss : 0.0182, val_loss : 0.0862, acc : 0.969\n","Epoch 48, loss : 0.0177, val_loss : 0.0847, acc : 0.969\n","Epoch 49, loss : 0.0172, val_loss : 0.0831, acc : 0.969\n","Epoch 50, loss : 0.0168, val_loss : 0.0814, acc : 0.969\n","Epoch 51, loss : 0.0163, val_loss : 0.0797, acc : 0.969\n","Epoch 52, loss : 0.0159, val_loss : 0.0783, acc : 0.969\n","Epoch 53, loss : 0.0155, val_loss : 0.0770, acc : 0.969\n","Epoch 54, loss : 0.0151, val_loss : 0.0761, acc : 0.969\n","Epoch 55, loss : 0.0148, val_loss : 0.0752, acc : 0.969\n","Epoch 56, loss : 0.0144, val_loss : 0.0742, acc : 0.969\n","Epoch 57, loss : 0.0141, val_loss : 0.0733, acc : 0.969\n","Epoch 58, loss : 0.0137, val_loss : 0.0725, acc : 0.969\n","Epoch 59, loss : 0.0134, val_loss : 0.0719, acc : 0.969\n","Epoch 60, loss : 0.0131, val_loss : 0.0713, acc : 0.969\n","Epoch 61, loss : 0.0128, val_loss : 0.0707, acc : 0.969\n","Epoch 62, loss : 0.0125, val_loss : 0.0701, acc : 0.969\n","Epoch 63, loss : 0.0122, val_loss : 0.0694, acc : 0.969\n","Epoch 64, loss : 0.0120, val_loss : 0.0688, acc : 0.969\n","Epoch 65, loss : 0.0117, val_loss : 0.0683, acc : 0.969\n","Epoch 66, loss : 0.0115, val_loss : 0.0678, acc : 0.969\n","Epoch 67, loss : 0.0112, val_loss : 0.0672, acc : 0.969\n","Epoch 68, loss : 0.0110, val_loss : 0.0667, acc : 0.969\n","Epoch 69, loss : 0.0107, val_loss : 0.0662, acc : 0.969\n","Epoch 70, loss : 0.0105, val_loss : 0.0657, acc : 0.969\n","Epoch 71, loss : 0.0103, val_loss : 0.0651, acc : 0.969\n","Epoch 72, loss : 0.0100, val_loss : 0.0647, acc : 0.969\n","Epoch 73, loss : 0.0098, val_loss : 0.0641, acc : 0.969\n","Epoch 74, loss : 0.0096, val_loss : 0.0636, acc : 0.969\n","Epoch 75, loss : 0.0094, val_loss : 0.0632, acc : 0.969\n","Epoch 76, loss : 0.0093, val_loss : 0.0628, acc : 0.969\n","Epoch 77, loss : 0.0091, val_loss : 0.0624, acc : 0.969\n","Epoch 78, loss : 0.0090, val_loss : 0.0619, acc : 0.969\n","Epoch 79, loss : 0.0088, val_loss : 0.0616, acc : 0.969\n","Epoch 80, loss : 0.0087, val_loss : 0.0613, acc : 0.969\n","Epoch 81, loss : 0.0085, val_loss : 0.0610, acc : 0.969\n","Epoch 82, loss : 0.0084, val_loss : 0.0606, acc : 0.969\n","Epoch 83, loss : 0.0083, val_loss : 0.0603, acc : 0.969\n","Epoch 84, loss : 0.0081, val_loss : 0.0599, acc : 0.969\n","Epoch 85, loss : 0.0080, val_loss : 0.0596, acc : 0.969\n","Epoch 86, loss : 0.0079, val_loss : 0.0594, acc : 0.969\n","Epoch 87, loss : 0.0078, val_loss : 0.0592, acc : 0.969\n","Epoch 88, loss : 0.0077, val_loss : 0.0589, acc : 0.969\n","Epoch 89, loss : 0.0075, val_loss : 0.0587, acc : 0.969\n","Epoch 90, loss : 0.0074, val_loss : 0.0583, acc : 0.969\n","Epoch 91, loss : 0.0073, val_loss : 0.0579, acc : 0.969\n","Epoch 92, loss : 0.0072, val_loss : 0.0575, acc : 0.969\n","Epoch 93, loss : 0.0071, val_loss : 0.0572, acc : 0.969\n","Epoch 94, loss : 0.0070, val_loss : 0.0569, acc : 0.969\n","Epoch 95, loss : 0.0069, val_loss : 0.0566, acc : 0.969\n","Epoch 96, loss : 0.0068, val_loss : 0.0564, acc : 0.969\n","Epoch 97, loss : 0.0067, val_loss : 0.0560, acc : 0.969\n","Epoch 98, loss : 0.0066, val_loss : 0.0557, acc : 0.969\n","Epoch 99, loss : 0.0065, val_loss : 0.0554, acc : 0.969\n","test_acc : 0.950\n"]}]},{"cell_type":"markdown","source":["## [Problem 3] Create a model of Iris using all three types of objective variables"],"metadata":{"id":"88Z1hNmN_Osy"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","df = pd.read_csv(\"Iris.csv\")\n","df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\") | (df[\"Species\"] == \"Iris-setosa\")]\n","y = df[\"Species\"]\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","X = np.array(X)\n","y = np.array(y)\n","y[y == \"Iris-versicolor\"] = 0\n","y[y == \"Iris-virginica\"] = 1\n","y[y == \"Iris-setosa\"] = 2\n","print(y)\n","y = y.astype(np.int64)[:, np.newaxis]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","y_train_one_hot = enc.fit_transform(y_train)\n","y_val_one_hot = enc.transform(y_val)\n","y_test_one_hot = enc.transform(y_test)\n","mmsc = MinMaxScaler()\n","X_train = mmsc.fit_transform(X_train)\n","X_test = mmsc.transform(X_test)\n","X_val = mmsc.transform(X_val)\n","# print(y_train_one_hot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_8a-T1R_Vnd","outputId":"6951f07a-36e3-4938-d1d7-0cddf5dd7662","executionInfo":{"status":"ok","timestamp":1716275901792,"user_tz":-480,"elapsed":6,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1]\n"]}]},{"cell_type":"code","source":["class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.intc)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]\n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","\n","# ハイパーパラメータの設定\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 100\n","\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","\n","#Network change\n","n_classes = 3\n","\n","#determine the shape of the network inputs and outputs\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","\n","loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n","correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n","\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n","\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    tf.random.set_random_seed(0)\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    print(type(x))\n","    print(type( weights['w1']))\n","    layer_1 = tf.add(tf.matmul(x,  weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    return layer_output\n","\n","# ネットワーク構造の読み込み\n","logits = example_net(X)\n","\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","\n","# 推定結果\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","# variableの初期化\n","init = tf.global_variables_initializer()\n","\n","\n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","        total_loss /= n_samples\n","        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n","    print(\"test_acc : {:.3f}\".format(test_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XskpKCkApQk","outputId":"d9a0dd93-9af5-4584-ddf6-e28290d661c2","executionInfo":{"status":"ok","timestamp":1716275925963,"user_tz":-480,"elapsed":2340,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'tensorflow.python.framework.ops.SymbolicTensor'>\n","<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n","Epoch 0, loss : 1.7987, val_loss : 14.7636, acc : 0.444\n","Epoch 1, loss : 1.2412, val_loss : 9.2677, acc : 0.431\n","Epoch 2, loss : 0.7166, val_loss : 5.2194, acc : 0.611\n","Epoch 3, loss : 0.4205, val_loss : 3.4381, acc : 0.667\n","Epoch 4, loss : 0.2684, val_loss : 2.1337, acc : 0.667\n","Epoch 5, loss : 0.1845, val_loss : 1.4346, acc : 0.722\n","Epoch 6, loss : 0.1488, val_loss : 1.0783, acc : 0.778\n","Epoch 7, loss : 0.1272, val_loss : 0.9443, acc : 0.847\n","Epoch 8, loss : 0.1129, val_loss : 0.8756, acc : 0.833\n","Epoch 9, loss : 0.1016, val_loss : 0.8028, acc : 0.833\n","Epoch 10, loss : 0.0919, val_loss : 0.7375, acc : 0.833\n","Epoch 11, loss : 0.0831, val_loss : 0.6720, acc : 0.833\n","Epoch 12, loss : 0.0745, val_loss : 0.6075, acc : 0.847\n","Epoch 13, loss : 0.0663, val_loss : 0.5457, acc : 0.847\n","Epoch 14, loss : 0.0586, val_loss : 0.4917, acc : 0.861\n","Epoch 15, loss : 0.0514, val_loss : 0.4502, acc : 0.875\n","Epoch 16, loss : 0.0446, val_loss : 0.4147, acc : 0.875\n","Epoch 17, loss : 0.0382, val_loss : 0.3815, acc : 0.889\n","Epoch 18, loss : 0.0325, val_loss : 0.3492, acc : 0.903\n","Epoch 19, loss : 0.0273, val_loss : 0.3205, acc : 0.903\n","Epoch 20, loss : 0.0228, val_loss : 0.2979, acc : 0.903\n","Epoch 21, loss : 0.0190, val_loss : 0.2798, acc : 0.903\n","Epoch 22, loss : 0.0157, val_loss : 0.2718, acc : 0.903\n","Epoch 23, loss : 0.0131, val_loss : 0.2723, acc : 0.903\n","Epoch 24, loss : 0.0111, val_loss : 0.2696, acc : 0.903\n","Epoch 25, loss : 0.0098, val_loss : 0.2665, acc : 0.917\n","Epoch 26, loss : 0.0088, val_loss : 0.2629, acc : 0.903\n","Epoch 27, loss : 0.0081, val_loss : 0.2588, acc : 0.903\n","Epoch 28, loss : 0.0076, val_loss : 0.2547, acc : 0.903\n","Epoch 29, loss : 0.0072, val_loss : 0.2512, acc : 0.903\n","Epoch 30, loss : 0.0069, val_loss : 0.2474, acc : 0.903\n","Epoch 31, loss : 0.0066, val_loss : 0.2437, acc : 0.903\n","Epoch 32, loss : 0.0064, val_loss : 0.2410, acc : 0.903\n","Epoch 33, loss : 0.0062, val_loss : 0.2383, acc : 0.903\n","Epoch 34, loss : 0.0060, val_loss : 0.2352, acc : 0.917\n","Epoch 35, loss : 0.0058, val_loss : 0.2332, acc : 0.917\n","Epoch 36, loss : 0.0057, val_loss : 0.2311, acc : 0.917\n","Epoch 37, loss : 0.0056, val_loss : 0.2278, acc : 0.917\n","Epoch 38, loss : 0.0054, val_loss : 0.2247, acc : 0.917\n","Epoch 39, loss : 0.0053, val_loss : 0.2227, acc : 0.917\n","Epoch 40, loss : 0.0052, val_loss : 0.2203, acc : 0.917\n","Epoch 41, loss : 0.0051, val_loss : 0.2178, acc : 0.917\n","Epoch 42, loss : 0.0050, val_loss : 0.2163, acc : 0.917\n","Epoch 43, loss : 0.0049, val_loss : 0.2145, acc : 0.917\n","Epoch 44, loss : 0.0049, val_loss : 0.2123, acc : 0.917\n","Epoch 45, loss : 0.0048, val_loss : 0.2111, acc : 0.917\n","Epoch 46, loss : 0.0047, val_loss : 0.2096, acc : 0.917\n","Epoch 47, loss : 0.0046, val_loss : 0.2075, acc : 0.917\n","Epoch 48, loss : 0.0046, val_loss : 0.2059, acc : 0.917\n","Epoch 49, loss : 0.0045, val_loss : 0.2051, acc : 0.917\n","Epoch 50, loss : 0.0044, val_loss : 0.2040, acc : 0.917\n","Epoch 51, loss : 0.0044, val_loss : 0.2030, acc : 0.917\n","Epoch 52, loss : 0.0043, val_loss : 0.2015, acc : 0.917\n","Epoch 53, loss : 0.0042, val_loss : 0.2005, acc : 0.917\n","Epoch 54, loss : 0.0042, val_loss : 0.2001, acc : 0.917\n","Epoch 55, loss : 0.0041, val_loss : 0.1990, acc : 0.917\n","Epoch 56, loss : 0.0041, val_loss : 0.1981, acc : 0.917\n","Epoch 57, loss : 0.0040, val_loss : 0.1978, acc : 0.917\n","Epoch 58, loss : 0.0040, val_loss : 0.1976, acc : 0.917\n","Epoch 59, loss : 0.0039, val_loss : 0.1977, acc : 0.917\n","Epoch 60, loss : 0.0039, val_loss : 0.1967, acc : 0.917\n","Epoch 61, loss : 0.0038, val_loss : 0.1957, acc : 0.917\n","Epoch 62, loss : 0.0038, val_loss : 0.1954, acc : 0.917\n","Epoch 63, loss : 0.0037, val_loss : 0.1938, acc : 0.917\n","Epoch 64, loss : 0.0037, val_loss : 0.1925, acc : 0.917\n","Epoch 65, loss : 0.0036, val_loss : 0.1911, acc : 0.917\n","Epoch 66, loss : 0.0035, val_loss : 0.1900, acc : 0.917\n","Epoch 67, loss : 0.0034, val_loss : 0.1885, acc : 0.917\n","Epoch 68, loss : 0.0034, val_loss : 0.1878, acc : 0.917\n","Epoch 69, loss : 0.0033, val_loss : 0.1871, acc : 0.917\n","Epoch 70, loss : 0.0033, val_loss : 0.1856, acc : 0.917\n","Epoch 71, loss : 0.0033, val_loss : 0.1853, acc : 0.917\n","Epoch 72, loss : 0.0032, val_loss : 0.1829, acc : 0.917\n","Epoch 73, loss : 0.0032, val_loss : 0.1807, acc : 0.917\n","Epoch 74, loss : 0.0032, val_loss : 0.1806, acc : 0.917\n","Epoch 75, loss : 0.0031, val_loss : 0.1799, acc : 0.917\n","Epoch 76, loss : 0.0031, val_loss : 0.1788, acc : 0.917\n","Epoch 77, loss : 0.0031, val_loss : 0.1781, acc : 0.931\n","Epoch 78, loss : 0.0031, val_loss : 0.1774, acc : 0.931\n","Epoch 79, loss : 0.0030, val_loss : 0.1767, acc : 0.931\n","Epoch 80, loss : 0.0030, val_loss : 0.1758, acc : 0.931\n","Epoch 81, loss : 0.0030, val_loss : 0.1749, acc : 0.931\n","Epoch 82, loss : 0.0030, val_loss : 0.1747, acc : 0.931\n","Epoch 83, loss : 0.0029, val_loss : 0.1741, acc : 0.931\n","Epoch 84, loss : 0.0029, val_loss : 0.1728, acc : 0.931\n","Epoch 85, loss : 0.0029, val_loss : 0.1724, acc : 0.931\n","Epoch 86, loss : 0.0029, val_loss : 0.1720, acc : 0.931\n","Epoch 87, loss : 0.0029, val_loss : 0.1712, acc : 0.931\n","Epoch 88, loss : 0.0028, val_loss : 0.1712, acc : 0.931\n","Epoch 89, loss : 0.0028, val_loss : 0.1707, acc : 0.931\n","Epoch 90, loss : 0.0028, val_loss : 0.1696, acc : 0.931\n","Epoch 91, loss : 0.0028, val_loss : 0.1688, acc : 0.931\n","Epoch 92, loss : 0.0028, val_loss : 0.1681, acc : 0.931\n","Epoch 93, loss : 0.0027, val_loss : 0.1671, acc : 0.931\n","Epoch 94, loss : 0.0027, val_loss : 0.1666, acc : 0.931\n","Epoch 95, loss : 0.0027, val_loss : 0.1663, acc : 0.931\n","Epoch 96, loss : 0.0027, val_loss : 0.1662, acc : 0.931\n","Epoch 97, loss : 0.0026, val_loss : 0.1642, acc : 0.931\n","Epoch 98, loss : 0.0026, val_loss : 0.1591, acc : 0.931\n","Epoch 99, loss : 0.0025, val_loss : 0.1603, acc : 0.931\n","test_acc : 1.000\n"]}]},{"cell_type":"markdown","source":["## [Problem 4] Create a House Prices model"],"metadata":{"id":"Z7NhaTALadrx"}},{"cell_type":"code","source":["dataset_path =\"train.csv\"\n","df = pd.read_csv(dataset_path)\n","y = df[\"SalePrice\"]\n","X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n","y = np.array(y)\n","X = np.array(X)\n","y = y.astype(np.intc)[:, np.newaxis]\n","y = np.log(y)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","mmsc = MinMaxScaler()\n","X_train = mmsc.fit_transform(X_train)\n","X_test = mmsc.transform(X_test)\n","X_val = mmsc.transform(X_val)"],"metadata":{"id":"7Cn9nWZVkmqx","executionInfo":{"status":"ok","timestamp":1716276069509,"user_tz":-480,"elapsed":331,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.intc)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]\n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","\n","# ハイパーパラメータの設定\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 100\n","\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 1\n","\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    tf.random.set_random_seed(0)\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    return layer_output\n","\n","# ネットワーク構造の読み込み\n","logits = example_net(X)\n","\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","\n","# 推定結果\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","# variableの初期化\n","init = tf.global_variables_initializer()\n","\n","\n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","        total_loss /= n_samples\n","        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JI7i6J-0ltrf","outputId":"bdb892b1-fffc-4a63-b62a-642b612e738c","executionInfo":{"status":"ok","timestamp":1716276105117,"user_tz":-480,"elapsed":14867,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, loss : -84.8883, val_loss : -1896.1158, acc : 1.000\n","Epoch 1, loss : -338.4815, val_loss : -5162.2349, acc : 1.000\n","Epoch 2, loss : -784.2259, val_loss : -10958.3311, acc : 1.000\n","Epoch 3, loss : -1529.2498, val_loss : -20174.1523, acc : 1.000\n","Epoch 4, loss : -2694.7960, val_loss : -34578.4961, acc : 1.000\n","Epoch 5, loss : -4464.4653, val_loss : -55459.2227, acc : 1.000\n","Epoch 6, loss : -6905.9824, val_loss : -83562.1484, acc : 1.000\n","Epoch 7, loss : -10133.2577, val_loss : -119973.2344, acc : 1.000\n","Epoch 8, loss : -14234.8151, val_loss : -165604.1562, acc : 1.000\n","Epoch 9, loss : -19289.7532, val_loss : -220975.4062, acc : 1.000\n","Epoch 10, loss : -25343.4574, val_loss : -286596.1562, acc : 1.000\n","Epoch 11, loss : -32448.8381, val_loss : -362985.9062, acc : 1.000\n","Epoch 12, loss : -40655.8530, val_loss : -450624.0312, acc : 1.000\n","Epoch 13, loss : -50010.3698, val_loss : -549949.5625, acc : 1.000\n","Epoch 14, loss : -60554.3883, val_loss : -661364.5625, acc : 1.000\n","Epoch 15, loss : -72326.3893, val_loss : -785237.8125, acc : 1.000\n","Epoch 16, loss : -85361.7026, val_loss : -921908.4375, acc : 1.000\n","Epoch 17, loss : -99692.8988, val_loss : -1071689.2500, acc : 1.000\n","Epoch 18, loss : -115350.0780, val_loss : -1234870.6250, acc : 1.000\n","Epoch 19, loss : -132361.1886, val_loss : -1411722.0000, acc : 1.000\n","Epoch 20, loss : -150752.2944, val_loss : -1602495.7500, acc : 1.000\n","Epoch 21, loss : -170547.7863, val_loss : -1807428.3750, acc : 1.000\n","Epoch 22, loss : -191770.6346, val_loss : -2026743.0000, acc : 1.000\n","Epoch 23, loss : -214442.5748, val_loss : -2260651.5000, acc : 1.000\n","Epoch 24, loss : -238584.2053, val_loss : -2509355.0000, acc : 1.000\n","Epoch 25, loss : -264215.2099, val_loss : -2773046.7500, acc : 1.000\n","Epoch 26, loss : -291354.5131, val_loss : -3051911.5000, acc : 1.000\n","Epoch 27, loss : -320020.2770, val_loss : -3346128.2500, acc : 1.000\n","Epoch 28, loss : -350230.1435, val_loss : -3655871.7500, acc : 1.000\n","Epoch 29, loss : -382001.2682, val_loss : -3981310.0000, acc : 1.000\n","Epoch 30, loss : -415350.3233, val_loss : -4322608.0000, acc : 1.000\n","Epoch 31, loss : -450293.7021, val_loss : -4679930.5000, acc : 1.000\n","Epoch 32, loss : -486847.5329, val_loss : -5053435.0000, acc : 1.000\n","Epoch 33, loss : -525027.6836, val_loss : -5443280.5000, acc : 1.000\n","Epoch 34, loss : -564849.8608, val_loss : -5849623.0000, acc : 1.000\n","Epoch 35, loss : -606329.6055, val_loss : -6272618.0000, acc : 1.000\n","Epoch 36, loss : -649482.3838, val_loss : -6712419.5000, acc : 1.000\n","Epoch 37, loss : -694323.5974, val_loss : -7169180.5000, acc : 1.000\n","Epoch 38, loss : -740868.5878, val_loss : -7643055.0000, acc : 1.000\n","Epoch 39, loss : -789132.6542, val_loss : -8134195.5000, acc : 1.000\n","Epoch 40, loss : -839131.1279, val_loss : -8642756.0000, acc : 1.000\n","Epoch 41, loss : -890879.3346, val_loss : -9168890.0000, acc : 1.000\n","Epoch 42, loss : -944392.6312, val_loss : -9712750.0000, acc : 1.000\n","Epoch 43, loss : -999686.4872, val_loss : -10274492.0000, acc : 1.000\n","Epoch 44, loss : -1056776.1799, val_loss : -10854271.0000, acc : 1.000\n","Epoch 45, loss : -1115677.2302, val_loss : -11452239.0000, acc : 1.000\n","Epoch 46, loss : -1176405.2955, val_loss : -12068552.0000, acc : 1.000\n","Epoch 47, loss : -1238975.8340, val_loss : -12703370.0000, acc : 1.000\n","Epoch 48, loss : -1303404.5921, val_loss : -13356846.0000, acc : 1.000\n","Epoch 49, loss : -1369707.2056, val_loss : -14029140.0000, acc : 1.000\n","Epoch 50, loss : -1437899.5310, val_loss : -14720409.0000, acc : 1.000\n","Epoch 51, loss : -1507997.3158, val_loss : -15430810.0000, acc : 1.000\n","Epoch 52, loss : -1580016.6188, val_loss : -16160507.0000, acc : 1.000\n","Epoch 53, loss : -1653973.3769, val_loss : -16909656.0000, acc : 1.000\n","Epoch 54, loss : -1729883.3737, val_loss : -17678420.0000, acc : 1.000\n","Epoch 55, loss : -1807763.2184, val_loss : -18466962.0000, acc : 1.000\n","Epoch 56, loss : -1887629.1242, val_loss : -19275446.0000, acc : 1.000\n","Epoch 57, loss : -1969497.0064, val_loss : -20104026.0000, acc : 1.000\n","Epoch 58, loss : -2053383.0985, val_loss : -20952872.0000, acc : 1.000\n","Epoch 59, loss : -2139304.0171, val_loss : -21822152.0000, acc : 1.000\n","Epoch 60, loss : -2227276.1199, val_loss : -22712014.0000, acc : 1.000\n","Epoch 61, loss : -2317315.6253, val_loss : -23622640.0000, acc : 1.000\n","Epoch 62, loss : -2409439.2527, val_loss : -24554188.0000, acc : 1.000\n","Epoch 63, loss : -2503663.4754, val_loss : -25506822.0000, acc : 1.000\n","Epoch 64, loss : -2600004.8672, val_loss : -26480712.0000, acc : 1.000\n","Epoch 65, loss : -2698480.1949, val_loss : -27476024.0000, acc : 1.000\n","Epoch 66, loss : -2799105.6210, val_loss : -28492920.0000, acc : 1.000\n","Epoch 67, loss : -2901898.3876, val_loss : -29531578.0000, acc : 1.000\n","Epoch 68, loss : -3006875.5782, val_loss : -30592154.0000, acc : 1.000\n","Epoch 69, loss : -3114052.8030, val_loss : -31674820.0000, acc : 1.000\n","Epoch 70, loss : -3223447.8672, val_loss : -32779744.0000, acc : 1.000\n","Epoch 71, loss : -3335076.8094, val_loss : -33907096.0000, acc : 1.000\n","Epoch 72, loss : -3448957.2034, val_loss : -35057036.0000, acc : 1.000\n","Epoch 73, loss : -3565105.2291, val_loss : -36229744.0000, acc : 1.000\n","Epoch 74, loss : -3683538.0385, val_loss : -37425380.0000, acc : 1.000\n","Epoch 75, loss : -3804272.8715, val_loss : -38644124.0000, acc : 1.000\n","Epoch 76, loss : -3927326.3212, val_loss : -39886136.0000, acc : 1.000\n","Epoch 77, loss : -4052715.2313, val_loss : -41151588.0000, acc : 1.000\n","Epoch 78, loss : -4180456.7281, val_loss : -42440652.0000, acc : 1.000\n","Epoch 79, loss : -4310567.6831, val_loss : -43753488.0000, acc : 1.000\n","Epoch 80, loss : -4443064.7666, val_loss : -45090276.0000, acc : 1.000\n","Epoch 81, loss : -4577965.8630, val_loss : -46451188.0000, acc : 1.000\n","Epoch 82, loss : -4715286.8437, val_loss : -47836384.0000, acc : 1.000\n","Epoch 83, loss : -4855045.2291, val_loss : -49246032.0000, acc : 1.000\n","Epoch 84, loss : -4997257.8758, val_loss : -50680320.0000, acc : 1.000\n","Epoch 85, loss : -5141941.8501, val_loss : -52139392.0000, acc : 1.000\n","Epoch 86, loss : -5289114.0942, val_loss : -53623440.0000, acc : 1.000\n","Epoch 87, loss : -5438791.5974, val_loss : -55132624.0000, acc : 1.000\n","Epoch 88, loss : -5590991.2034, val_loss : -56667104.0000, acc : 1.000\n","Epoch 89, loss : -5745730.3555, val_loss : -58227080.0000, acc : 1.000\n","Epoch 90, loss : -5903025.9143, val_loss : -59812704.0000, acc : 1.000\n","Epoch 91, loss : -6062894.6424, val_loss : -61424128.0000, acc : 1.000\n","Epoch 92, loss : -6225353.2548, val_loss : -63061552.0000, acc : 1.000\n","Epoch 93, loss : -6390419.8330, val_loss : -64725128.0000, acc : 1.000\n","Epoch 94, loss : -6558110.1884, val_loss : -66415024.0000, acc : 1.000\n","Epoch 95, loss : -6728441.6745, val_loss : -68131432.0000, acc : 1.000\n","Epoch 96, loss : -6901431.3019, val_loss : -69874496.0000, acc : 1.000\n","Epoch 97, loss : -7077095.6831, val_loss : -71644392.0000, acc : 1.000\n","Epoch 98, loss : -7255452.5268, val_loss : -73441304.0000, acc : 1.000\n","Epoch 99, loss : -7436518.9208, val_loss : -75265384.0000, acc : 1.000\n","test_acc : 1.000\n"]}]},{"cell_type":"markdown","source":["## [Problem 5] Create an MNIST model"],"metadata":{"id":"PGJ4LGt3l5nN"}},{"cell_type":"code","source":["(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","X_train = X_train.reshape(-1, 784)\n","X_test = X_test.reshape(-1, 784)\n","X_train = X_train.astype(np.float32)\n","X_test = X_test.astype(np.float32)\n","X_train /= 255\n","X_test /= 255\n","y_train = y_train.astype(np.intc)[:, np.newaxis]\n","y_test = y_test.astype(np.intc)[:, np.newaxis]\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","y_train_one_hot = enc.fit_transform(y_train)\n","y_val_one_hot = enc.transform(y_val)\n","y_test_one_hot = enc.transform(y_test)\n","mmsc = MinMaxScaler()\n","X_train = mmsc.fit_transform(X_train)\n","X_test = mmsc.transform(X_test)\n","X_val = mmsc.transform(X_val)\n","# print(y_train_one_hot)"],"metadata":{"id":"bbNlWlIRmHr0","executionInfo":{"status":"ok","timestamp":1716276158224,"user_tz":-480,"elapsed":1258,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.intc)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]\n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","\n","# ハイパーパラメータの設定\n","learning_rate = 0.001\n","batch_size = 10\n","num_epochs = 10\n","\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","\n","#Network change\n","n_classes = 10\n","\n","#determine the shape of the network inputs and outputs\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","\n","loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n","correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n","\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n","\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    tf.random.set_random_seed(0)\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    print(type(x))\n","    print(type( weights['w1']))\n","    layer_1 = tf.add(tf.matmul(x,  weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    return layer_output\n","\n","# ネットワーク構造の読み込み\n","logits = example_net(X)\n","\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","\n","# 推定結果\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","# variableの初期化\n","init = tf.global_variables_initializer()\n","\n","\n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","        total_loss /= n_samples\n","        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n","    print(\"test_acc : {:.3f}\".format(test_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xO-Z5IsmYDr","outputId":"c0495708-36ba-4b3f-996c-dafff617a310","executionInfo":{"status":"ok","timestamp":1716276338324,"user_tz":-480,"elapsed":81502,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'tensorflow.python.framework.ops.SymbolicTensor'>\n","<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n","Epoch 0, loss : 0.4614, val_loss : 1.0630, acc : 0.959\n","Epoch 1, loss : 0.0665, val_loss : 0.4636, acc : 0.968\n","Epoch 2, loss : 0.0325, val_loss : 0.2776, acc : 0.972\n","Epoch 3, loss : 0.0195, val_loss : 0.2087, acc : 0.974\n","Epoch 4, loss : 0.0134, val_loss : 0.1575, acc : 0.978\n","Epoch 5, loss : 0.0101, val_loss : 0.1293, acc : 0.980\n","Epoch 6, loss : 0.0081, val_loss : 0.1193, acc : 0.981\n","Epoch 7, loss : 0.0067, val_loss : 0.1051, acc : 0.982\n","Epoch 8, loss : 0.0057, val_loss : 0.0983, acc : 0.983\n","Epoch 9, loss : 0.0050, val_loss : 0.0962, acc : 0.983\n","test_acc : 0.984\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HKQeC1RGCEM4"},"execution_count":null,"outputs":[]}]}