{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1alxwZ_071FSxbrsp6t6GgZfDJHBp8c-W","timestamp":1716270103923}],"authorship_tag":"ABX9TyMGJo1A+OIDPLJMmCIMeNgO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## [Problem 1] Creating a one-dimensional convolutional layer class that limits the number of channels to one"],"metadata":{"id":"urgml8GghGWx"}},{"cell_type":"code","source":["import numpy as np\n","import math\n","from keras.datasets import mnist\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"y7p_s9CxhC3w","executionInfo":{"status":"ok","timestamp":1716273168935,"user_tz":-480,"elapsed":7410,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Activation functions\n","class Sigmoid:\n","    def forward(self, A):\n","        self.A = A\n","        return self.sigmoid(A)\n","    def backward(self, dZ):\n","        _sig = self.sigmoid(self.A)\n","        return dZ * (1 - _sig)*_sig\n","    def sigmoid(self, X):\n","        return 1 / (1 + np.exp(-X))\n","\n","class Tanh:\n","    def forward(self, A):\n","        self.A = A\n","        return np.tanh(A)\n","    def backward(self, dZ):\n","        return dZ * (1 - (np.tanh(self.A))**2)\n","\n","class Softmax:\n","    def forward(self, X):\n","        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n","        return self.Z\n","    def backward(self, Y):\n","        self.loss = self.loss_func(Y)\n","        return self.Z - Y\n","    def loss_func(self, Y, Z=None):\n","        if Z is None:\n","            Z = self.Z\n","        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n","\n","class ReLU:\n","    def forward(self, A):\n","        self.A = A\n","        return np.clip(A, 0, None)\n","    def backward(self, dZ):\n","        return dZ * np.clip(np.sign(self.A), 0, None)\n","\n","# FC = Neural network\n","class FC:\n","    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer,activation):\n","        self.optimizer = optimizer\n","        self.W = initializer.W(n_nodes1, n_nodes2)\n","        self.B = initializer.B(n_nodes2)\n","        self.activation = activation\n","    def forward(self, X):\n","        self.X = X\n","        A = X@self.W + self.B\n","        return self.activation.forward(A)\n","    def backward(self, dA):\n","        dA = self.activation.backward(dA)\n","        dZ = dA@self.W.T\n","        self.dB = np.sum(dA, axis=0)\n","        self.dW = self.X.T@dA\n","        self.optimizer.update(self)\n","        return dZ\n","\n","# Defining a Weight Initialization Class\n","class XavierInitializer:\n","    def W(self, n_nodes1, n_nodes2):\n","        self.sigma = math.sqrt(1 / n_nodes1)\n","        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n","        return W\n","    def B(self, n_nodes2):\n","        B = self.sigma * np.random.randn(n_nodes2)\n","        return B\n","\n","class HeInitializer():\n","    def W(self, n_nodes1, n_nodes2):\n","        self.sigma = math.sqrt(2 / n_nodes1)\n","        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n","        return W\n","    def B(self, n_nodes2):\n","        B = self.sigma * np.random.randn(n_nodes2)\n","        return B\n","\n","class SimpleInitializer:\n","    def __init__(self, sigma):\n","        self.sigma = sigma\n","    def W(self, *shape):\n","        W = self.sigma * np.random.randn(*shape)\n","        return W\n","    def B(self, *shape):\n","        B = self.sigma * np.random.randn(*shape)\n","        return B\n","\n","class SimpleInitializerConv1d:\n","    def __init__(self, sigma):\n","        self.sigma = sigma\n","    def W(self, *shape):\n","        W = self.sigma * np.random.randn(*shape)\n","        return W\n","    def B(self, *shape):\n","        B = self.sigma * np.random.randn(*shape)\n","        return B\n","\n","# Defining Gradient Update Class\n","class SGD:\n","    def __init__(self, lr):\n","        self.lr = lr\n","    def update(self, layer):\n","        layer.W -= self.lr * layer.dW\n","        layer.B -= self.lr * layer.dB\n","        return\n","\n","class AdaGrad:\n","    def __init__(self, lr):\n","        self.lr = lr\n","        self.HW = 1\n","        self.HB = 1\n","    def update(self, layer):\n","        self.HW += layer.dW**2\n","        self.HB += layer.dB**2\n","        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n","        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB\n","\n","# Defining a mini-batch generation iterator\n","class GetMiniBatch:\n","    def __init__(self, X, y, batch_size = 20, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self._X = X[shuffle_index]\n","        self._y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self._X[p0:p1], self._y[p0:p1]\n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self._X[p0:p1], self._y[p0:p1]"],"metadata":{"id":"67exQRhGhM9p","executionInfo":{"status":"ok","timestamp":1716273168936,"user_tz":-480,"elapsed":14,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class SimpleConv1d():\n","    '''\n","    1d conv layer\n","    Parameters\n","    -----------------\n","    n_nodes1 : int\n","        Number of nodes in the previous layer\n","    n_nodes 2 : int\n","        Number of nodes in later layers\n","    Initializer : Instances of initialization method\n","    Optimizer : Instances of optimization method\n","    '''\n","    def __init__(self, output_channel, input_channel, filter_size, padding = 0, stride = 1, initializer = None, optimizer = None, activation = None):\n","        self.initializer = initializer\n","        self.optimizer = optimizer\n","        self.activation = activation\n","\n","        self.W = self.initializer.W(output_channel, input_channel, filter_size)\n","        self.B = self.initializer.B(output_channel)\n","\n","    def output_size_calculation(self, n_in, filter_size, padding=0, stride=1):\n","        \"\"\"\n","        Calculate output size after 1d convolution\n","\n","        Parameters\n","        -----------------\n","        n_in: Input size\n","        F: filter size\n","        P: padding number\n","        S: stride number\n","\n","        Return\n","        -----------------\n","        n_out: size of output\n","        \"\"\"\n","        n_out = int((n_in + 2*padding - filter_size) / stride + 1)\n","        return n_out\n","\n","    def forward(self, X):\n","        '''\n","        Calculate forward propagation\n","        Parameters\n","        --------------\n","        x : ndarray shape with (batch_size, n_nodes1)\n","            training feature\n","\n","        returns\n","        ---------------\n","        A : ndarray shape with (batch_size, n_nodes2)\n","        '''\n","        self.X = X\n","        N,INC, Feature = X.shape\n","        OCH, INC, FS = self.W.shape\n","        OUT = self.output_size_calculation(Feature, FS, 0,1)\n","        self.size = N,INC,OCH,FS,OUT\n","        A = np.zeros([N,OCH,OUT])\n","\n","        for n in range(N):\n","            for och in range(OCH):\n","                for ich in range(INC):\n","                    for m in range(OUT):\n","                        A[n,och,m] += np.sum(X[n, ich,m:m+FS]*self.W[och,ich,:])\n","\n","        A += self.B[:,None]\n","        A = self.activation.forward(A)\n","\n","        return A\n","\n","    def backward(self, dZ):\n","        '''\n","        Calculate backward propagation\n","        Parameters\n","        --------------\n","        x: ndarray\n","            training feature\n","        w: ndarray\n","            weight\n","        da: ndarray\n","            backpropagation value\n","        '''\n","        dA = self.activation.backward(dZ)\n","        self.dB = np.mean(np.sum(dA, axis=2), axis = 0)\n","\n","        self.dW = np.zeros(self.W.shape)\n","        dZ = np.zeros(self.X.shape)\n","\n","        N,INC,OCH,FS,OUT = self.size\n","        for n in range(N):\n","            for och in range(OCH):\n","                for ich in range(INC):\n","                    for fs in range(FS):\n","                        for m in range(OUT):\n","                            self.dW[och,ich,fs] += self.X[n,ich,fs+m]*dA[n,och,m]\n","                            dZ[n,ich,fs+m] += self.W[och,ich,fs]*dA[n,och,m]\n","\n","        self = self.optimizer.update(self)\n","\n","        return dZ"],"metadata":{"id":"wrygxTk6xqYN","executionInfo":{"status":"ok","timestamp":1716273168936,"user_tz":-480,"elapsed":11,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## [Problem 2] Output size calculation after one-dimensional convolution"],"metadata":{"id":"8zHqon4QhXW5"}},{"cell_type":"code","source":["def output_size_calculation( n_in, filter_size, padding=0, stride=1):\n","  \"\"\"\n","  Calculate output size after 1d convolution\n","\n","  Parameters\n","  -----------------\n","  n_in: Input size\n","  F: filter size\n","  P: padding number\n","  S: stride number\n","\n","  Return\n","  -----------------\n","  n_out: size of output\n","  \"\"\"\n","  n_out = int((n_in + 2*padding - filter_size) / stride + 1)\n","  return n_out\n","\n","a = output_size_calculation(4,3,0,1)\n","print(\"output:\", a)"],"metadata":{"id":"U9rNRlhlhWsU","executionInfo":{"status":"ok","timestamp":1716273168937,"user_tz":-480,"elapsed":12,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b830ac2e-7f6b-46ee-c5fa-5668c5da57d3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["output: 2\n"]}]},{"cell_type":"markdown","source":["## [Problem 3] Experiment of one-dimensional convolutional layer with small array"],"metadata":{"id":"H9F_LCjFhpB6"}},{"cell_type":"code","source":["X = np.array([1,2,3,4])\n","w = np.array([3,5,7])\n","b = np.array([1])\n","\n","# forward propagation\n","a = np.zeros(a)\n","for i in range(len(a)):\n","    x_temp = X[i:i+len(w)]\n","    a[i] = np.sum(x_temp*w)+b\n","print(a)\n","\n","#  back propagation\n","delta_a = np.array([10,20])\n","delta_b = np.sum(delta_a)\n","print(delta_b)\n","\n","delta_w = np.zeros([len(w)])\n","for i in range(len(w)):\n","    x_temp = X[i:i+len(delta_a)]\n","    delta_w[i] = np.sum(x_temp*delta_a)\n","print(delta_w)\n","\n","delta_x = np.zeros(len(X))\n","\n","for i in range(len(X)):\n","    zero = np.zeros(len(delta_a)-1)\n","    w_padded = np.concatenate([zero,w,zero], axis =0)\n","    w_temp = w_padded[i:i+len(delta_a)]\n","    delta_x[i] = np.sum(w_temp*delta_a[::-1])\n","print(delta_x)"],"metadata":{"id":"7QRGa86hhoOn","executionInfo":{"status":"ok","timestamp":1716273168937,"user_tz":-480,"elapsed":9,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6513809-0b69-4072-95a4-8a352397fa42"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[35. 50.]\n","30\n","[ 50.  80. 110.]\n","[ 30. 110. 170. 140.]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-8b6409922634>:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  a[i] = np.sum(x_temp*w)+b\n"]}]},{"cell_type":"markdown","source":["## [Problem 4] Creating a one-dimensional convolutional layer class that does not limit the number of channels"],"metadata":{"id":"jDwnEGeIh1w0"}},{"cell_type":"code","source":["class Scratch1dCNNClassifier:\n","    \"\"\"\n","    1d conv layer\n","    \"\"\"\n","    def __init__(self, NN, CNN, n_epoch = 10, n_batch = 5, verbose = False):\n","\n","        self.n_batch = n_batch\n","        self.n_epoch = n_epoch\n","        self.verbose = verbose\n","\n","        self.log_loss = np.zeros(self.n_epoch)\n","        self.log_acc = np.zeros(self.n_epoch)\n","        self.NN = NN\n","        self.CNN = CNN\n","    def loss_function(self, y, yt):\n","        delta = 1e-7\n","        temp = -np.mean(yt*np.log(y + delta))\n","        return temp\n","\n","    def accuracy(self, y, yt):\n","        return accuracy_score(y,yt)\n","\n","    def fit(self, X, y, X_val = False, y_val = False):\n","        \"\"\"\n","        Train a cnn classifier\n","\n","        Parameters\n","        ---------------\n","        X : ndarray shape with (n_samples, n_features)\n","            features of training data\n","        y : ndarray shape with (n_samples, )\n","            True label of training data\n","        X_val : ndarray shape with (n_samples, n_features)\n","            features of validation data\n","        y_val : ndarray shape with (n_samples, )\n","            True label of validation data\n","        \"\"\"\n","\n","        for epoch in range(self.n_epoch):\n","            self.loss = 0\n","            get_mini_batch = GetMiniBatch(X,y, batch_size=self.n_batch)\n","            for mini_X_train, mini_y_train in get_mini_batch:\n","                forward_data = mini_X_train.reshape(self.n_batch,1,-1)\n","                for layer in range(len(self.CNN)):\n","                    forward_data = self.CNN[layer].forward(forward_data)\n","\n","                record_shape = forward_data.shape\n","                forward_data = forward_data.reshape(self.n_batch, -1)\n","\n","                for layer in range(len(self.NN)):\n","                    forward_data = self.NN[layer].forward(forward_data)\n","\n","                Z = forward_data\n","\n","                backward_data = (Z - mini_y_train)/self.n_batch\n","                for layer in range(len(self.NN)-1,-1,-1):\n","                    backward_data = self.NN[layer].backward(backward_data)\n","\n","                backward_data = backward_data.reshape(record_shape)\n","\n","                for layer in range(len(self.CNN)-1,-1,-1):\n","                    backward_data = self.CNN[layer].backward(backward_data)\n","\n","                self.loss += self.loss_function(Z, mini_y_train)\n","\n","            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n","            self.log_acc[epoch] = self.accuracy(self.predict(X), np.argmax(y, axis = 1))\n","\n","            if self.verbose:\n","                print('epoch:{} loss:{} acc:{}'.format(epoch, self.loss/self.n_batch, self.log_acc[epoch]))\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Estimate using a neural network classifier\n","\n","        Parameters\n","        ---------------\n","        X : ndarray shape with (n_samples, n_features)\n","            sample of dataset\n","\n","        Returns\n","        ---------------\n","        pred : ndarray (n_samples, 1)\n","        \"\"\"\n","        pred_data = X[:, np.newaxis, :]\n","\n","        for layer in range(len(self.CNN)):\n","            pred_data = self.CNN[layer].forward(pred_data)\n","\n","        pred_data = pred_data.reshape(len(X),-1)\n","        for layer in range(len(self.CNN)):\n","            pred_data = self.NN[layer].forward(pred_data)\n","\n","        pred = np.argmax(pred_data, axis = 1)\n","        return pred"],"metadata":{"id":"fz_END8UhzS8","executionInfo":{"status":"ok","timestamp":1716273169693,"user_tz":-480,"elapsed":6,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["x = np.array([[1,2,3,4],[2,3,4,5]])\n","w = np.array([[[1,1,2],[2,1,1]], [[2,1,1],[1,1,1]], [[1,1,1],[1,1,1]]])\n","b = np.array([1,2,3])\n","\n","a = np.zeros([3, output_size_calculation(4,3,0,1)])\n","\n","for och in range(w.shape[0]):\n","    for ch in range(w.shape[1]):\n","        for m in range(a.shape[1]):\n","            a[och,m] += np.sum(x[ch, m:m+w.shape[2]]* w[och,ch,:])\n","\n","a += b[:,None]\n","print(\"print forward prop:\", a)\n","\n","delta_a = np.array([[9,11], [32,35],[52,56]])\n","\n","print(\"delta_a:\\n\", delta_a)\n","print(\"delta_a.shape:\\n\", delta_a.shape)\n","\n","delta_b = np.sum(delta_a, axis = 1)\n","print(\"delta_b: \", delta_b)\n","\n","delta_w = np.zeros([3,2,3])\n","\n","for och in range(w.shape[0]):\n","    for ich in range(w.shape[1]):\n","        for fs in range(w.shape[2]):\n","            for m in range(2):\n","                delta_w[och,ich,fs] += (x[ich, fs+m]*delta_a[och,m])\n","\n","print(\"delta_w:\\n\", delta_w)\n","\n","delta_x = np.zeros([2,4])\n","\n","for och in range(w.shape[0]):\n","    for ich in range(w.shape[1]):\n","        for fs in range(w.shape[2]):\n","            for m in range(2):\n","                delta_x[ich,fs+m] += w[och, ich,fs] * delta_a[och,m]\n","\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","print(\"X_train data shape: \", X_train.shape) # (60000, 28, 28)\n","print(\"X_test data shape: \", X_test.shape) # (10000, 28, 28)\n","\n","X_train = X_train.reshape(-1, 784)\n","X_test = X_test.reshape(-1, 784)\n","# Preprocessing\n","X_train = X_train.astype(np.float32)\n","X_test = X_test.astype(np.float32)\n","X_train /= 255\n","X_test /= 255\n","# the correct label is an integer from 0 to 9, but it is converted to a one-hot representation\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n","enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","y_train_one_hot = enc.fit_transform(y_train.reshape(-1,1))\n","y_val_one_hot = enc.fit_transform(y_val.reshape(-1,1))\n","print(\"Train dataset:\", X_train.shape) # (48000, 784)\n","print(\"Validation dataset:\", X_val.shape) # (12000, 784)\n","\n","NN = {0: FC(15640, 400, HeInitializer(), AdaGrad(0.01), Tanh()),\n","    1: FC(400, 200, HeInitializer(), AdaGrad(0.01), Tanh()),\n","    2: FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),}\n","\n","CNN = {0: SimpleConv1d(output_channel=20, input_channel=1, filter_size=3, padding=0, stride=1,\n","            initializer=SimpleInitializerConv1d(0.01), optimizer=SGD(0.01), activation=Sigmoid()),}\n","\n","cnn1d = Scratch1dCNNClassifier(NN=NN, CNN=CNN, n_epoch=10, n_batch=100, verbose = True)\n","cnn1d.fit(X_train[0:1000], y_train_one_hot[0:1000])\n","\n","y_pred = cnn1d.predict(X_val[0:500])\n","acc = accuracy_score(y_val[0:500], y_pred)\n","print(\"Accuracy:\", acc)"],"metadata":{"id":"8C1qfutxisZN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716276855232,"user_tz":-480,"elapsed":3685544,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}},"outputId":"3f4a27a7-bef7-4a53-cf17-8bec44e17195"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["print forward prop: [[21. 29.]\n"," [18. 25.]\n"," [18. 24.]]\n","delta_a:\n"," [[ 9 11]\n"," [32 35]\n"," [52 56]]\n","delta_a.shape:\n"," (3, 2)\n","delta_b:  [ 20  67 108]\n","delta_w:\n"," [[[ 31.  51.  71.]\n","  [ 51.  71.  91.]]\n","\n"," [[102. 169. 236.]\n","  [169. 236. 303.]]\n","\n"," [[164. 272. 380.]\n","  [272. 380. 488.]]]\n","X_train data shape:  (60000, 28, 28)\n","X_test data shape:  (10000, 28, 28)\n","Train dataset: (48000, 784)\n","Validation dataset: (12000, 784)\n","epoch:0 loss:0.02313325707012719 acc:0.0\n","epoch:1 loss:0.023122030272779343 acc:0.0\n","epoch:2 loss:0.023122106780330984 acc:0.0\n","epoch:3 loss:0.023122190251464145 acc:0.0\n","epoch:4 loss:0.02312226193960902 acc:0.0\n","epoch:5 loss:0.0231223224886822 acc:0.0\n","epoch:6 loss:0.023122373895788174 acc:0.0\n","epoch:7 loss:0.02312241801092183 acc:0.0\n","epoch:8 loss:0.02312245630365826 acc:0.0\n","epoch:9 loss:0.023122489900690477 acc:0.0\n","Accuracy: 0.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"u2IT6pQA2XeN","executionInfo":{"status":"ok","timestamp":1716276855234,"user_tz":-480,"elapsed":31,"user":{"displayName":"Batgerel M","userId":"12820710479957792879"}}},"execution_count":7,"outputs":[]}]}