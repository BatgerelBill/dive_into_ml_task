{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9761d2d3-c1ee-4cfa-8f6e-5684bf7d0cd6",
   "metadata": {},
   "source": [
    "## Problem1-6 Hypothetical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b9f0c5-ed01-4fb3-bdf3-5ce304aa73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, no_bias, verbose): \n",
    "        \n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = no_bias\n",
    "        self.verbose = verbose\n",
    "     \n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "      \n",
    "\n",
    "    # Problem6 Learning and Estimation\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias == True:\n",
    "            bias = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((bias, X))\n",
    "            if X_val is not None:\n",
    "                bias = np.ones((X_val.shape[0], 1))\n",
    "                X_val = np.hstack((bias, X_val))\n",
    "            self.coef_ = np.random.rand(X.shape[1])\n",
    "            self.coef_ = self.coef_.reshape(X.shape[1], 1)\n",
    "    \n",
    "\n",
    "        for epoch in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            self.loss[epoch] = np.mean((y-y_pred)**2)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                pred_val = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[epoch] = np.mean((y_val-pred_val)**2)\n",
    "                \n",
    "            self.coef_ = self._gradient_descent(X, (y_pred-y))\n",
    "           \n",
    "            if self.verbose == True:\n",
    "                print('{}-th epoch train loss {}'.format(epoch, self.loss[epoch]))\n",
    "                if X_val is not None:\n",
    "                    print('{}-th epoch val loss {}'.format(epoch, self.val_loss[epoch] ))\n",
    "\n",
    "\n",
    "    # Problem1 Hypothetical function\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        仮定関数の出力を計算する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形の仮定関数による推定結果\n",
    "        \"\"\"\n",
    "        pred = X @ self.coef_\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    # Problem2 Steepest descent\n",
    "    def _gradient_descent(self, X, error):\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            gradient = error*X[:, i]\n",
    "            self.coef_[i, :] = self.coef_[i, :] - self.lr * np.mean(gradient)\n",
    "\n",
    "        return self.coef_\n",
    "        \n",
    "\n",
    "    # Problem3 Estimated\n",
    "    def predict(self, X):\n",
    "        if self.bias == True:\n",
    "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
    "            X = np.hstack([bias, X])\n",
    "        pred_y = self._linear_hypothesis(X)\n",
    "        return pred_y\n",
    "\n",
    "    # Problem4 Mean squared error\n",
    "    def _mse(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "        \"\"\"\n",
    "        mse = np.mean((y-y_pred)**2)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "    # Problem5 Objective function\n",
    "    def _loss_func(self, pred, y):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        loss = self._mse(pred, y)/2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28332de6-0bea-439f-8edf-809b5e1700b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataset = pd.read_csv(\"../data/house/train.csv\")\n",
    "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
    "y = dataset.loc[:, ['SalePrice']]\n",
    "X = X.values\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "y = np.log(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c681e318-7ccb-4289-87b1-da9373cdfdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th epoch train loss 128.0876260111541\n",
      "0-th epoch val loss 127.65319555810936\n",
      "1-th epoch train loss 124.09771538261259\n",
      "1-th epoch val loss 123.68329946245066\n",
      "2-th epoch train loss 120.23264064078602\n",
      "2-th epoch val loss 119.83754637620845\n",
      "3-th epoch train loss 116.48850522377893\n",
      "3-th epoch val loss 116.11206240573635\n",
      "4-th epoch train loss 112.86153404985373\n",
      "4-th epoch val loss 112.50309441453614\n",
      "5-th epoch train loss 109.34806973240687\n",
      "5-th epoch val loss 109.00700626101676\n",
      "6-th epoch train loss 105.94456891284187\n",
      "6-th epoch val loss 105.62027515343614\n",
      "7-th epoch train loss 102.64759870766683\n",
      "7-th epoch val loss 102.33948811837651\n",
      "8-th epoch train loss 99.45383326625934\n",
      "8-th epoch val loss 99.16133857921731\n",
      "9-th epoch train loss 96.36005043585244\n",
      "9-th epoch val loss 96.08262304117999\n",
      "10-th epoch train loss 93.36312853040188\n",
      "10-th epoch val loss 93.10023787962588\n",
      "11-th epoch train loss 90.46004320009986\n",
      "11-th epoch val loss 90.21117622839145\n",
      "12-th epoch train loss 87.64786439840054\n",
      "12-th epoch val loss 87.41252496504546\n",
      "13-th epoch train loss 84.92375344352043\n",
      "13-th epoch val loss 84.70146179004948\n",
      "14-th epoch train loss 82.28496017147131\n",
      "14-th epoch val loss 82.07525239689735\n",
      "15-th epoch train loss 79.72882017777533\n",
      "15-th epoch val loss 79.53124773040028\n",
      "16-th epoch train loss 77.25275214509958\n",
      "16-th epoch val loss 77.06688133037228\n",
      "17-th epoch train loss 74.8542552541352\n",
      "17-th epoch val loss 74.67966675805631\n",
      "18-th epoch train loss 72.53090667512768\n",
      "18-th epoch val loss 72.36719510271429\n",
      "19-th epoch train loss 70.2803591375469\n",
      "19-th epoch val loss 70.1271325658844\n",
      "20-th epoch train loss 68.10033857546317\n",
      "20-th epoch val loss 67.95721812088671\n",
      "21-th epoch train loss 65.9886418462714\n",
      "21-th epoch val loss 65.85526124523386\n",
      "22-th epoch train loss 63.943134520479184\n",
      "22-th epoch val loss 63.819139723675754\n",
      "23-th epoch train loss 61.96174874034517\n",
      "23-th epoch val loss 61.84679751967911\n",
      "24-th epoch train loss 60.042481145223846\n",
      "24-th epoch val loss 59.93624271320975\n",
      "25-th epoch train loss 58.183390861538726\n",
      "25-th epoch val loss 58.085545502753504\n",
      "26-th epoch train loss 56.38259755537138\n",
      "26-th epoch val loss 56.29283626957423\n",
      "27-th epoch train loss 54.63827954571604\n",
      "27-th epoch val loss 54.556303702271244\n",
      "28-th epoch train loss 52.9486719765102\n",
      "28-th epoch val loss 52.87419297975788\n",
      "29-th epoch train loss 51.312065045610886\n",
      "29-th epoch val loss 51.244804010841705\n",
      "30-th epoch train loss 49.726802288942686\n",
      "30-th epoch val loss 49.66648972864352\n",
      "31-th epoch train loss 48.19127891809947\n",
      "31-th epoch val loss 48.13765443814735\n",
      "32-th epoch train loss 46.703940209734654\n",
      "32-th epoch val loss 46.656752215226426\n",
      "33-th epoch train loss 45.263279945127294\n",
      "33-th epoch val loss 45.222285355542134\n",
      "34-th epoch train loss 43.867838898360986\n",
      "34-th epoch val loss 43.83280287176256\n",
      "35-th epoch train loss 42.51620337160172\n",
      "35-th epoch val loss 42.48689903759572\n",
      "36-th epoch train loss 41.20700377600744\n",
      "36-th epoch val loss 41.18321197717931\n",
      "37-th epoch train loss 39.93891325684835\n",
      "37-th epoch val loss 39.920422298414536\n",
      "38-th epoch train loss 38.71064636146076\n",
      "38-th epoch val loss 38.69725176887518\n",
      "39-th epoch train loss 37.520957748700425\n",
      "39-th epoch val loss 37.512462032965885\n",
      "40-th epoch train loss 36.36864093860278\n",
      "40-th epoch val loss 36.36485336904507\n",
      "41-th epoch train loss 35.25252710099778\n",
      "41-th epoch val loss 35.25326348526755\n",
      "42-th epoch train loss 34.17148388186605\n",
      "42-th epoch val loss 34.17656635294101\n",
      "43-th epoch train loss 33.12441426626069\n",
      "43-th epoch val loss 33.133671076227834\n",
      "44-th epoch train loss 32.11025547665599\n",
      "44-th epoch val loss 32.123520797060436\n",
      "45-th epoch train loss 31.12797790561946\n",
      "45-th epoch val loss 31.14509163417306\n",
      "46-th epoch train loss 30.176584081738195\n",
      "46-th epoch val loss 30.197391655187758\n",
      "47-th epoch train loss 29.25510766776367\n",
      "47-th epoch val loss 29.279459880724673\n",
      "48-th epoch train loss 28.362612489971525\n",
      "48-th epoch val loss 28.390365319539423\n",
      "49-th epoch train loss 27.498191597763995\n",
      "49-th epoch val loss 27.52920603372117\n",
      "50-th epoch train loss 26.66096635257307\n",
      "50-th epoch val loss 26.6951082330149\n",
      "51-th epoch train loss 25.850085545151597\n",
      "51-th epoch val loss 25.88722539736099\n",
      "52-th epoch train loss 25.064724540368196\n",
      "52-th epoch val loss 25.10473742677301\n",
      "53-th epoch train loss 24.30408444864917\n",
      "53-th epoch val loss 24.34684981770232\n",
      "54-th epoch train loss 23.56739132323757\n",
      "54-th epoch val loss 23.612792865064485\n",
      "55-th epoch train loss 22.853895382465005\n",
      "55-th epoch val loss 22.90182088912809\n",
      "56-th epoch train loss 22.162870256257307\n",
      "56-th epoch val loss 22.213211486491712\n",
      "57-th epoch train loss 21.49361225611898\n",
      "57-th epoch val loss 21.546264804398536\n",
      "58-th epoch train loss 20.84543966786524\n",
      "58-th epoch val loss 20.900302837661954\n",
      "59-th epoch train loss 20.217692066393017\n",
      "59-th epoch val loss 20.274668747497586\n",
      "60-th epoch train loss 19.609729651804333\n",
      "60-th epoch val loss 19.668726201579556\n",
      "61-th epoch train loss 19.020932606217062\n",
      "61-th epoch val loss 19.081858734659836\n",
      "62-th epoch train loss 18.450700470618624\n",
      "62-th epoch val loss 18.513469129110174\n",
      "63-th epoch train loss 17.89845154113811\n",
      "63-th epoch val loss 17.962978814766014\n",
      "64-th epoch train loss 17.363622284132273\n",
      "64-th epoch val loss 17.4298272874712\n",
      "65-th epoch train loss 16.84566676949895\n",
      "65-th epoch val loss 16.913471545740876\n",
      "66-th epoch train loss 16.344056121650397\n",
      "66-th epoch val loss 16.413385544978333\n",
      "67-th epoch train loss 15.858277987596285\n",
      "67-th epoch val loss 15.929059668698901\n",
      "68-th epoch train loss 15.38783602160338\n",
      "68-th epoch val loss 15.460000216231116\n",
      "69-th epoch train loss 14.932249385915545\n",
      "69-th epoch val loss 15.00572890638199\n",
      "70-th epoch train loss 14.49105226703374\n",
      "70-th epoch val loss 14.565782396569002\n",
      "71-th epoch train loss 14.063793407071307\n",
      "71-th epoch val loss 14.139711816937073\n",
      "72-th epoch train loss 13.650035649714912\n",
      "72-th epoch val loss 13.727082318993686\n",
      "73-th epoch train loss 13.249355500336168\n",
      "73-th epoch val loss 13.32747263830998\n",
      "74-th epoch train loss 12.861342699813031\n",
      "74-th epoch val loss 12.940474670849547\n",
      "75-th epoch train loss 12.485599811633975\n",
      "75-th epoch val loss 12.565693062500499\n",
      "76-th epoch train loss 12.121741821871105\n",
      "76-th epoch val loss 12.202744811399464\n",
      "77-th epoch train loss 11.76939575162126\n",
      "77-th epoch val loss 11.851258882649029\n",
      "78-th epoch train loss 11.42820028152674\n",
      "78-th epoch val loss 11.510875835042544\n",
      "79-th epoch train loss 11.097805387999339\n",
      "79-th epoch val loss 11.181247459422284\n",
      "80-th epoch train loss 10.777871990783009\n",
      "80-th epoch val loss 10.862036428308464\n",
      "81-th epoch train loss 10.468071611502012\n",
      "81-th epoch val loss 10.552915956448128\n",
      "82-th epoch train loss 10.168086042852218\n",
      "82-th epoch val loss 10.253569471943605\n",
      "83-th epoch train loss 9.877607028104041\n",
      "83-th epoch val loss 9.963690297631006\n",
      "84-th epoch train loss 9.59633595059568\n",
      "84-th epoch val loss 9.682981342389459\n",
      "85-th epoch train loss 9.323983532905515\n",
      "85-th epoch val loss 9.41115480207167\n",
      "86-th epoch train loss 9.060269545402\n",
      "86-th epoch val loss 9.147931869756093\n",
      "87-th epoch train loss 8.804922523878957\n",
      "87-th epoch val loss 8.893042455030288\n",
      "88-th epoch train loss 8.557679495993172\n",
      "88-th epoch val loss 8.646224912024138\n",
      "89-th epoch train loss 8.318285716230072\n",
      "89-th epoch val loss 8.40722577592029\n",
      "90-th epoch train loss 8.086494409131756\n",
      "90-th epoch val loss 8.175799507677766\n",
      "91-th epoch train loss 7.862066520529966\n",
      "91-th epoch val loss 7.951708246712816\n",
      "92-th epoch train loss 7.644770476534584\n",
      "92-th epoch val loss 7.734721571289141\n",
      "93-th epoch train loss 7.434381950036\n",
      "93-th epoch val loss 7.524616266377275\n",
      "94-th epoch train loss 7.230683634487284\n",
      "94-th epoch val loss 7.321176098750447\n",
      "95-th epoch train loss 7.033465024739248\n",
      "95-th epoch val loss 7.124191599091415\n",
      "96-th epoch train loss 6.842522204708748\n",
      "96-th epoch val loss 6.933459850891906\n",
      "97-th epoch train loss 6.657657641667233\n",
      "97-th epoch val loss 6.748784285932986\n",
      "98-th epoch train loss 6.478679986943306\n",
      "98-th epoch val loss 6.569974486141312\n",
      "99-th epoch train loss 6.305403882839404\n",
      "99-th epoch val loss 6.396845991622634\n",
      "100-th epoch train loss 6.137649775569009\n",
      "100-th epoch val loss 6.2292201146800945\n",
      "101-th epoch train loss 5.975243734026762\n",
      "101-th epoch val loss 6.066923759630819\n",
      "102-th epoch train loss 5.818017274209729\n",
      "102-th epoch val loss 5.9097892482401475\n",
      "103-th epoch train loss 5.665807189113749\n",
      "103-th epoch val loss 5.757654150598505\n",
      "104-th epoch train loss 5.518455383934218\n",
      "104-th epoch val loss 5.610361121271271\n",
      "105-th epoch train loss 5.3758087164060395\n",
      "105-th epoch val loss 5.467757740557377\n",
      "106-th epoch train loss 5.237718842122607\n",
      "106-th epoch val loss 5.329696360697472\n",
      "107-th epoch train loss 5.104042064678628\n",
      "107-th epoch val loss 5.196033956877387\n",
      "108-th epoch train loss 4.974639190486499\n",
      "108-th epoch val loss 5.066631982877485\n",
      "109-th epoch train loss 4.849375388120575\n",
      "109-th epoch val loss 4.941356231223187\n",
      "110-th epoch train loss 4.728120052048226\n",
      "110-th epoch val loss 4.820076697696312\n",
      "111-th epoch train loss 4.610746670611001\n",
      "111-th epoch val loss 4.70266745007146\n",
      "112-th epoch train loss 4.497132698123402\n",
      "112-th epoch val loss 4.589006500945693\n",
      "113-th epoch train loss 4.387159430960995\n",
      "113-th epoch val loss 4.478975684533997\n",
      "114-th epoch train loss 4.280711887513486\n",
      "114-th epoch val loss 4.3724605373069405\n",
      "115-th epoch train loss 4.177678691882353\n",
      "115-th epoch val loss 4.269350182350817\n",
      "116-th epoch train loss 4.077951961206298\n",
      "116-th epoch val loss 4.169537217334212\n",
      "117-th epoch train loss 3.981427196501476\n",
      "117-th epoch val loss 4.0729176059686845\n",
      "118-th epoch train loss 3.888003176906943\n",
      "118-th epoch val loss 3.9793905728546126\n",
      "119-th epoch train loss 3.7975818572291993\n",
      "119-th epoch val loss 3.8888585016067356\n",
      "120-th epoch train loss 3.7100682686830178\n",
      "120-th epoch val loss 3.8012268361571873\n",
      "121-th epoch train loss 3.625370422728897\n",
      "121-th epoch val loss 3.7164039851369775\n",
      "122-th epoch train loss 3.543399217910678\n",
      "122-th epoch val loss 3.634301229240006\n",
      "123-th epoch train loss 3.464068349599761\n",
      "123-th epoch val loss 3.5548326314766316\n",
      "124-th epoch train loss 3.3872942225553566\n",
      "124-th epoch val loss 3.477914950226763\n",
      "125-th epoch train loss 3.312995866212984\n",
      "125-th epoch val loss 3.4034675550051947\n",
      "126-th epoch train loss 3.241094852616184\n",
      "126-th epoch val loss 3.3314123448546944\n",
      "127-th epoch train loss 3.1715152169090524\n",
      "127-th epoch val loss 3.261673669284905\n",
      "128-th epoch train loss 3.104183380309766\n",
      "128-th epoch val loss 3.194178251677741\n",
      "129-th epoch train loss 3.039028075487764\n",
      "129-th epoch val loss 3.1288551150823705\n",
      "130-th epoch train loss 2.975980274269676\n",
      "130-th epoch val loss 3.065635510325365\n",
      "131-th epoch train loss 2.9149731176013614\n",
      "131-th epoch val loss 3.0044528463637654\n",
      "132-th epoch train loss 2.855941847695784\n",
      "132-th epoch val loss 2.9452426228112407\n",
      "133-th epoch train loss 2.7988237422985303\n",
      "133-th epoch val loss 2.887942364569551\n",
      "134-th epoch train loss 2.7435580510050004\n",
      "134-th epoch val loss 2.8324915584997163\n",
      "135-th epoch train loss 2.6900859335652583\n",
      "135-th epoch val loss 2.7788315920692885\n",
      "136-th epoch train loss 2.638350400114625\n",
      "136-th epoch val loss 2.726905693914151\n",
      "137-th epoch train loss 2.5882962532699594\n",
      "137-th epoch val loss 2.6766588762551646\n",
      "138-th epoch train loss 2.5398700320334324\n",
      "138-th epoch val loss 2.6280378791118166\n",
      "139-th epoch train loss 2.4930199574474945\n",
      "139-th epoch val loss 2.5809911162568793\n",
      "140-th epoch train loss 2.4476958799463917\n",
      "140-th epoch val loss 2.5354686228577985\n",
      "141-th epoch train loss 2.403849228351357\n",
      "141-th epoch val loss 2.491422004752218\n",
      "142-th epoch train loss 2.3614329604582234\n",
      "142-th epoch val loss 2.44880438930671\n",
      "143-th epoch train loss 2.320401515167803\n",
      "143-th epoch val loss 2.4075703778093493\n",
      "144-th epoch train loss 2.280710766110939\n",
      "144-th epoch val loss 2.3676759993483083\n",
      "145-th epoch train loss 2.2423179767216084\n",
      "145-th epoch val loss 2.3290786661301532\n",
      "146-th epoch train loss 2.205181756712933\n",
      "146-th epoch val loss 2.2917371301929426\n",
      "147-th epoch train loss 2.169262019912349\n",
      "147-th epoch val loss 2.255611441470654\n",
      "148-th epoch train loss 2.1345199434135558\n",
      "148-th epoch val loss 2.2206629071667945\n",
      "149-th epoch train loss 2.1009179280041788\n",
      "149-th epoch val loss 2.186854052396397\n",
      "150-th epoch train loss 2.068419559829357\n",
      "150-th epoch val loss 2.1541485820568216\n",
      "151-th epoch train loss 2.0369895732527215\n",
      "151-th epoch val loss 2.1225113438890753\n",
      "152-th epoch train loss 2.0065938148774234\n",
      "152-th epoch val loss 2.0919082926925245\n",
      "153-th epoch train loss 1.9771992086910168\n",
      "153-th epoch val loss 2.0623064556570156\n",
      "154-th epoch train loss 1.9487737222991721\n",
      "154-th epoch val loss 2.0336738987775864\n",
      "155-th epoch train loss 1.9212863342142321\n",
      "155-th epoch val loss 2.0059796943179924\n",
      "156-th epoch train loss 1.8947070021657355\n",
      "156-th epoch val loss 1.9791938892903502\n",
      "157-th epoch train loss 1.8690066324010162\n",
      "157-th epoch val loss 1.9532874749192082\n",
      "158-th epoch train loss 1.8441570499450035\n",
      "158-th epoch val loss 1.9282323570593436\n",
      "159-th epoch train loss 1.8201309697892982\n",
      "159-th epoch val loss 1.9040013275375387\n",
      "160-th epoch train loss 1.79690196898155\n",
      "160-th epoch val loss 1.8805680363895414\n",
      "161-th epoch train loss 1.7744444595870417\n",
      "161-th epoch val loss 1.8579069649642597\n",
      "162-th epoch train loss 1.7527336624952712\n",
      "162-th epoch val loss 1.8359933998681697\n",
      "163-th epoch train loss 1.7317455820451741\n",
      "163-th epoch val loss 1.81480340772371\n",
      "164-th epoch train loss 1.7114569814434597\n",
      "164-th epoch val loss 1.7943138107163004\n",
      "165-th epoch train loss 1.6918453589512956\n",
      "165-th epoch val loss 1.774502162905356\n",
      "166-th epoch train loss 1.6728889248153833\n",
      "166-th epoch val loss 1.755346727275492\n",
      "167-th epoch train loss 1.654566578920203\n",
      "167-th epoch val loss 1.7368264535048195\n",
      "168-th epoch train loss 1.636857889138914\n",
      "168-th epoch val loss 1.71892095642796\n",
      "169-th epoch train loss 1.6197430703611295\n",
      "169-th epoch val loss 1.701610495172122\n",
      "170-th epoch train loss 1.603202964176428\n",
      "170-th epoch val loss 1.6848759529452284\n",
      "171-th epoch train loss 1.587219019193152\n",
      "171-th epoch val loss 1.6686988174557587\n",
      "172-th epoch train loss 1.571773271972665\n",
      "172-th epoch val loss 1.653061161944601\n",
      "173-th epoch train loss 1.5568483285598662\n",
      "173-th epoch val loss 1.6379456268098225\n",
      "174-th epoch train loss 1.5424273465913492\n",
      "174-th epoch val loss 1.6233354018058455\n",
      "175-th epoch train loss 1.5284940179631845\n",
      "175-th epoch val loss 1.6092142087991332\n",
      "176-th epoch train loss 1.5150325520408536\n",
      "176-th epoch val loss 1.595566285062992\n",
      "177-th epoch train loss 1.5020276593944228\n",
      "177-th epoch val loss 1.5823763670946938\n",
      "178-th epoch train loss 1.489464536042558\n",
      "178-th epoch val loss 1.5696296749386094\n",
      "179-th epoch train loss 1.4773288481894953\n",
      "179-th epoch val loss 1.5573118969995576\n",
      "180-th epoch train loss 1.4656067174395926\n",
      "180-th epoch val loss 1.5454091753310908\n",
      "181-th epoch train loss 1.454284706474541\n",
      "181-th epoch val loss 1.533908091383876\n",
      "182-th epoch train loss 1.4433498051788065\n",
      "182-th epoch val loss 1.522795652199833\n",
      "183-th epoch train loss 1.4327894171992956\n",
      "183-th epoch val loss 1.5120592770380938\n",
      "184-th epoch train loss 1.422591346925703\n",
      "184-th epoch val loss 1.501686784419336\n",
      "185-th epoch train loss 1.4127437868784005\n",
      "185-th epoch val loss 1.4916663795754084\n",
      "186-th epoch train loss 1.4032353054911457\n",
      "186-th epoch val loss 1.4819866422916108\n",
      "187-th epoch train loss 1.3940548352762798\n",
      "187-th epoch val loss 1.4726365151293692\n",
      "188-th epoch train loss 1.3851916613604776\n",
      "188-th epoch val loss 1.4636052920174345\n",
      "189-th epoch train loss 1.376635410379477\n",
      "189-th epoch val loss 1.4548826072000993\n",
      "190-th epoch train loss 1.3683760397205755\n",
      "190-th epoch val loss 1.4464584245312877\n",
      "191-th epoch train loss 1.36040382710204\n",
      "191-th epoch val loss 1.438323027103725\n",
      "192-th epoch train loss 1.3527093604789011\n",
      "192-th epoch val loss 1.43046700720272\n",
      "193-th epoch train loss 1.345283528264947\n",
      "193-th epoch val loss 1.4228812565744302\n",
      "194-th epoch train loss 1.3381175098610347\n",
      "194-th epoch val loss 1.4155569569987942\n",
      "195-th epoch train loss 1.3312027664801533\n",
      "195-th epoch val loss 1.408485571157607\n",
      "196-th epoch train loss 1.3245310322599724\n",
      "196-th epoch val loss 1.401658833788535\n",
      "197-th epoch train loss 1.3180943056538952\n",
      "197-th epoch val loss 1.3950687431161417\n",
      "198-th epoch train loss 1.3118848410919137\n",
      "198-th epoch val loss 1.3887075525512618\n",
      "199-th epoch train loss 1.3058951409028396\n",
      "199-th epoch val loss 1.382567762650356\n",
      "200-th epoch train loss 1.300117947489745\n",
      "200-th epoch val loss 1.3766421133267235\n",
      "201-th epoch train loss 1.2945462357507023\n",
      "201-th epoch val loss 1.370923576305705\n",
      "202-th epoch train loss 1.2891732057371579\n",
      "202-th epoch val loss 1.3654053478162562\n",
      "203-th epoch train loss 1.2839922755425177\n",
      "203-th epoch val loss 1.3600808415115127\n",
      "204-th epoch train loss 1.2789970744137453\n",
      "204-th epoch val loss 1.3549436816111884\n",
      "205-th epoch train loss 1.274181436079008\n",
      "205-th epoch val loss 1.3499876962588788\n",
      "206-th epoch train loss 1.2695393922846194\n",
      "206-th epoch val loss 1.3452069110875602\n",
      "207-th epoch train loss 1.2650651665347352\n",
      "207-th epoch val loss 1.3405955429867766\n",
      "208-th epoch train loss 1.2607531680274653\n",
      "208-th epoch val loss 1.336147994065216\n",
      "209-th epoch train loss 1.2565979857812601\n",
      "209-th epoch val loss 1.3318588458025633\n",
      "210-th epoch train loss 1.2525943829456372\n",
      "210-th epoch val loss 1.3277228533847336\n",
      "211-th epoch train loss 1.2487372912904593\n",
      "211-th epoch val loss 1.3237349402167315\n",
      "212-th epoch train loss 1.2450218058682145\n",
      "212-th epoch val loss 1.3198901926076123\n",
      "213-th epoch train loss 1.2414431798438583\n",
      "213-th epoch val loss 1.3161838546221414\n",
      "214-th epoch train loss 1.2379968194870035\n",
      "214-th epoch val loss 1.312611323093963\n",
      "215-th epoch train loss 1.2346782793213618\n",
      "215-th epoch val loss 1.3091681427952138\n",
      "216-th epoch train loss 1.2314832574265386\n",
      "216-th epoch val loss 1.3058500017577093\n",
      "217-th epoch train loss 1.2284075908873981\n",
      "217-th epoch val loss 1.3026527267409453\n",
      "218-th epoch train loss 1.225447251386398\n",
      "218-th epoch val loss 1.2995722788423407\n",
      "219-th epoch train loss 1.2225983409344119\n",
      "219-th epoch val loss 1.296604749245268\n",
      "220-th epoch train loss 1.2198570877357147\n",
      "220-th epoch val loss 1.293746355100565\n",
      "221-th epoch train loss 1.2172198421829312\n",
      "221-th epoch val loss 1.2909934355373531\n",
      "222-th epoch train loss 1.2146830729778795\n",
      "222-th epoch val loss 1.2883424477991225\n",
      "223-th epoch train loss 1.212243363374377\n",
      "223-th epoch val loss 1.285789963501164\n",
      "224-th epoch train loss 1.2098974075391857\n",
      "224-th epoch val loss 1.283332665005552\n",
      "225-th epoch train loss 1.2076420070274052\n",
      "225-th epoch val loss 1.2809673419100065\n",
      "226-th epoch train loss 1.2054740673687299\n",
      "226-th epoch val loss 1.2786908876470708\n",
      "227-th epoch train loss 1.2033905947611\n",
      "227-th epoch val loss 1.2765002961901477\n",
      "228-th epoch train loss 1.2013886928683886\n",
      "228-th epoch val loss 1.2743926588630665\n",
      "229-th epoch train loss 1.1994655597188622\n",
      "229-th epoch val loss 1.272365161249924\n",
      "230-th epoch train loss 1.1976184847012672\n",
      "230-th epoch val loss 1.2704150802020793\n",
      "231-th epoch train loss 1.1958448456554793\n",
      "231-th epoch val loss 1.2685397809392462\n",
      "232-th epoch train loss 1.1941421060547623\n",
      "232-th epoch val loss 1.2667367142417585\n",
      "233-th epoch train loss 1.1925078122767594\n",
      "233-th epoch val loss 1.265003413731133\n",
      "234-th epoch train loss 1.1909395909604459\n",
      "234-th epoch val loss 1.263337493236186\n",
      "235-th epoch train loss 1.1894351464463466\n",
      "235-th epoch val loss 1.261736644242012\n",
      "236-th epoch train loss 1.1879922582974145\n",
      "236-th epoch val loss 1.2601986334192437\n",
      "237-th epoch train loss 1.186608778898036\n",
      "237-th epoch val loss 1.2587213002310667\n",
      "238-th epoch train loss 1.1852826311287235\n",
      "238-th epoch val loss 1.257302554615563\n",
      "239-th epoch train loss 1.184011806114124\n",
      "239-th epoch val loss 1.2559403747410292\n",
      "240-th epoch train loss 1.182794361042036\n",
      "240-th epoch val loss 1.2546328048319673\n",
      "241-th epoch train loss 1.1816284170512235\n",
      "241-th epoch val loss 1.2533779530635576\n",
      "242-th epoch train loss 1.1805121571858659\n",
      "242-th epoch val loss 1.2521739895224542\n",
      "243-th epoch train loss 1.1794438244145444\n",
      "243-th epoch val loss 1.2510191442318233\n",
      "244-th epoch train loss 1.178421719711765\n",
      "244-th epoch val loss 1.249911705238631\n",
      "245-th epoch train loss 1.177444200200034\n",
      "245-th epoch val loss 1.2488500167612062\n",
      "246-th epoch train loss 1.1765096773506032\n",
      "246-th epoch val loss 1.2478324773952092\n",
      "247-th epoch train loss 1.1756166152410361\n",
      "247-th epoch val loss 1.2468575383761658\n",
      "248-th epoch train loss 1.1747635288678233\n",
      "248-th epoch val loss 1.2459237018968004\n",
      "249-th epoch train loss 1.173948982512307\n",
      "249-th epoch val loss 1.2450295194774452\n",
      "250-th epoch train loss 1.173171588158256\n",
      "250-th epoch val loss 1.2441735903878686\n",
      "251-th epoch train loss 1.172430003959458\n",
      "251-th epoch val loss 1.2433545601189038\n",
      "252-th epoch train loss 1.1717229327557757\n",
      "252-th epoch val loss 1.242571118902321\n",
      "253-th epoch train loss 1.1710491206361278\n",
      "253-th epoch val loss 1.241822000277434\n",
      "254-th epoch train loss 1.1704073555469392\n",
      "254-th epoch val loss 1.2411059797029684\n",
      "255-th epoch train loss 1.1697964659446205\n",
      "255-th epoch val loss 1.240421873212773\n",
      "256-th epoch train loss 1.1692153194907022\n",
      "256-th epoch val loss 1.2397685361140094\n",
      "257-th epoch train loss 1.1686628217882824\n",
      "257-th epoch val loss 1.2391448617264726\n",
      "258-th epoch train loss 1.1681379151584907\n",
      "258-th epoch val loss 1.238549780161766\n",
      "259-th epoch train loss 1.1676395774557105\n",
      "259-th epoch val loss 1.2379822571410697\n",
      "260-th epoch train loss 1.1671668209203454\n",
      "260-th epoch val loss 1.2374412928502994\n",
      "261-th epoch train loss 1.1667186910679472\n",
      "261-th epoch val loss 1.236925920831476\n",
      "262-th epoch train loss 1.166294265613567\n",
      "262-th epoch val loss 1.2364352069091769\n",
      "263-th epoch train loss 1.165892653430219\n",
      "263-th epoch val loss 1.2359682481509577\n",
      "264-th epoch train loss 1.1655129935403847\n",
      "264-th epoch val loss 1.2355241718606915\n",
      "265-th epoch train loss 1.1651544541395245\n",
      "265-th epoch val loss 1.2351021346037758\n",
      "266-th epoch train loss 1.164816231650586\n",
      "266-th epoch val loss 1.2347013212632219\n",
      "267-th epoch train loss 1.1644975498085326\n",
      "267-th epoch val loss 1.234320944125647\n",
      "268-th epoch train loss 1.1641976587739566\n",
      "268-th epoch val loss 1.2339602419962368\n",
      "269-th epoch train loss 1.1639158342748528\n",
      "269-th epoch val loss 1.2336184793417597\n",
      "270-th epoch train loss 1.1636513767756713\n",
      "270-th epoch val loss 1.233294945460758\n",
      "271-th epoch train loss 1.163403610672795\n",
      "271-th epoch val loss 1.2329889536800647\n",
      "272-th epoch train loss 1.1631718835156026\n",
      "272-th epoch val loss 1.2326998405768097\n",
      "273-th epoch train loss 1.1629555652523165\n",
      "273-th epoch val loss 1.2324269652251199\n",
      "274-th epoch train loss 1.162754047499856\n",
      "274-th epoch val loss 1.2321697084667382\n",
      "275-th epoch train loss 1.162566742836939\n",
      "275-th epoch val loss 1.231927472204805\n",
      "276-th epoch train loss 1.1623930841197\n",
      "276-th epoch val loss 1.2316996787200785\n",
      "277-th epoch train loss 1.1622325238191142\n",
      "277-th epoch val loss 1.2314857700088795\n",
      "278-th epoch train loss 1.1620845333795455\n",
      "278-th epoch val loss 1.2312852071420954\n",
      "279-th epoch train loss 1.161948602597748\n",
      "279-th epoch val loss 1.2310974696445565\n",
      "280-th epoch train loss 1.1618242390216753\n",
      "280-th epoch val loss 1.2309220548941626\n",
      "281-th epoch train loss 1.161710967368483\n",
      "281-th epoch val loss 1.2307584775401301\n",
      "282-th epoch train loss 1.1616083289611074\n",
      "282-th epoch val loss 1.2306062689397623\n",
      "283-th epoch train loss 1.161515881182843\n",
      "283-th epoch val loss 1.2304649766131515\n",
      "284-th epoch train loss 1.161433196949346\n",
      "284-th epoch val loss 1.230334163715258\n",
      "285-th epoch train loss 1.1613598641975196\n",
      "285-th epoch val loss 1.2302134085248129\n",
      "286-th epoch train loss 1.1612954853907445\n",
      "286-th epoch val loss 1.2301023039495167\n",
      "287-th epoch train loss 1.1612396770399371\n",
      "287-th epoch val loss 1.2300004570470195\n",
      "288-th epoch train loss 1.1611920692399456\n",
      "288-th epoch val loss 1.2299074885611858\n",
      "289-th epoch train loss 1.1611523052207835\n",
      "289-th epoch val loss 1.2298230324731627\n",
      "290-th epoch train loss 1.1611200409132518\n",
      "290-th epoch val loss 1.2297467355667835\n",
      "291-th epoch train loss 1.1610949445284773\n",
      "291-th epoch val loss 1.229678257007858\n",
      "292-th epoch train loss 1.1610766961509391\n",
      "292-th epoch val loss 1.2296172679369053\n",
      "293-th epoch train loss 1.1610649873445533\n",
      "293-th epoch val loss 1.2295634510749132\n",
      "294-th epoch train loss 1.1610595207714018\n",
      "294-th epoch val loss 1.2295165003417046\n",
      "295-th epoch train loss 1.1610600098227093\n",
      "295-th epoch val loss 1.2294761204865223\n",
      "296-th epoch train loss 1.1610661782616758\n",
      "296-th epoch val loss 1.2294420267304382\n",
      "297-th epoch train loss 1.1610777598777975\n",
      "297-th epoch val loss 1.2294139444202177\n",
      "298-th epoch train loss 1.1610944981523037\n",
      "298-th epoch val loss 1.229391608693278\n",
      "299-th epoch train loss 1.1611161459343646\n",
      "299-th epoch val loss 1.229374764153388\n",
      "300-th epoch train loss 1.1611424651277262\n",
      "300-th epoch val loss 1.2293631645567737\n",
      "301-th epoch train loss 1.161173226387444\n",
      "301-th epoch val loss 1.229356572508293\n",
      "302-th epoch train loss 1.1612082088263922\n",
      "302-th epoch val loss 1.2293547591673721\n",
      "303-th epoch train loss 1.1612471997312441\n",
      "303-th epoch val loss 1.229357503963387\n",
      "304-th epoch train loss 1.161289994287617\n",
      "304-th epoch val loss 1.2293645943201932\n",
      "305-th epoch train loss 1.1613363953140998\n",
      "305-th epoch val loss 1.2293758253895162\n",
      "306-th epoch train loss 1.1613862130048689\n",
      "306-th epoch val loss 1.2293909997929182\n",
      "307-th epoch train loss 1.1614392646806397\n",
      "307-th epoch val loss 1.2294099273720809\n",
      "308-th epoch train loss 1.1614953745476622\n",
      "308-th epoch val loss 1.2294324249471205\n",
      "309-th epoch train loss 1.1615543734645357\n",
      "309-th epoch val loss 1.2294583160827044\n",
      "310-th epoch train loss 1.1616160987165678\n",
      "310-th epoch val loss 1.2294874308617028\n",
      "311-th epoch train loss 1.1616803937974562\n",
      "311-th epoch val loss 1.2295196056661482\n",
      "312-th epoch train loss 1.1617471081980528\n",
      "312-th epoch val loss 1.229554682965271\n",
      "313-th epoch train loss 1.1618160972019846\n",
      "313-th epoch val loss 1.2295925111103754\n",
      "314-th epoch train loss 1.1618872216879177\n",
      "314-th epoch val loss 1.2296329441363536\n",
      "315-th epoch train loss 1.1619603479382512\n",
      "315-th epoch val loss 1.2296758415696216\n",
      "316-th epoch train loss 1.1620353474540333\n",
      "316-th epoch val loss 1.229721068242266\n",
      "317-th epoch train loss 1.1621120967759069\n",
      "317-th epoch val loss 1.2297684941122162\n",
      "318-th epoch train loss 1.1621904773108904\n",
      "318-th epoch val loss 1.2298179940892469\n",
      "319-th epoch train loss 1.162270375164808\n",
      "319-th epoch val loss 1.2298694478666192\n",
      "320-th epoch train loss 1.1623516809801882\n",
      "320-th epoch val loss 1.2299227397581893\n",
      "321-th epoch train loss 1.1624342897794586\n",
      "321-th epoch val loss 1.2299777585408098\n",
      "322-th epoch train loss 1.1625181008132677\n",
      "322-th epoch val loss 1.2300343973018502\n",
      "323-th epoch train loss 1.162603017413769\n",
      "323-th epoch val loss 1.2300925532916787\n",
      "324-th epoch train loss 1.16268894685271\n",
      "324-th epoch val loss 1.2301521277809486\n",
      "325-th epoch train loss 1.1627758002041726\n",
      "325-th epoch val loss 1.2302130259225306\n",
      "326-th epoch train loss 1.1628634922118182\n",
      "326-th epoch val loss 1.2302751566179475\n",
      "327-th epoch train loss 1.1629519411604874\n",
      "327-th epoch val loss 1.2303384323881672\n",
      "328-th epoch train loss 1.1630410687520252\n",
      "328-th epoch val loss 1.2304027692486155\n",
      "329-th epoch train loss 1.1631307999851872\n",
      "329-th epoch val loss 1.2304680865882707\n",
      "330-th epoch train loss 1.1632210630395021\n",
      "330-th epoch val loss 1.2305343070527188\n",
      "331-th epoch train loss 1.1633117891629607\n",
      "331-th epoch val loss 1.230601356431029\n",
      "332-th epoch train loss 1.1634029125634133\n",
      "332-th epoch val loss 1.2306691635463418\n",
      "333-th epoch train loss 1.1634943703035467\n",
      "333-th epoch val loss 1.2307376601500417\n",
      "334-th epoch train loss 1.163586102199344\n",
      "334-th epoch val loss 1.2308067808194065\n",
      "335-th epoch train loss 1.163678050721894\n",
      "335-th epoch val loss 1.2308764628586135\n",
      "336-th epoch train loss 1.1637701609024598\n",
      "336-th epoch val loss 1.2309466462030079\n",
      "337-th epoch train loss 1.1638623802406947\n",
      "337-th epoch val loss 1.231017273326514\n",
      "338-th epoch train loss 1.163954658615904\n",
      "338-th epoch val loss 1.2310882891521053\n",
      "339-th epoch train loss 1.164046948201258\n",
      "339-th epoch val loss 1.2311596409652195\n",
      "340-th epoch train loss 1.164139203380858\n",
      "340-th epoch val loss 1.2312312783300357\n",
      "341-th epoch train loss 1.1642313806695674\n",
      "341-th epoch val loss 1.2313031530085181\n",
      "342-th epoch train loss 1.1643234386355144\n",
      "342-th epoch val loss 1.2313752188821345\n",
      "343-th epoch train loss 1.1644153378251851\n",
      "343-th epoch val loss 1.2314474318761697\n",
      "344-th epoch train loss 1.1645070406910198\n",
      "344-th epoch val loss 1.2315197498865473\n",
      "345-th epoch train loss 1.164598511521434\n",
      "345-th epoch val loss 1.231592132709079\n",
      "346-th epoch train loss 1.1646897163731853\n",
      "346-th epoch val loss 1.2316645419710643\n",
      "347-th epoch train loss 1.1647806230060087\n",
      "347-th epoch val loss 1.2317369410651657\n",
      "348-th epoch train loss 1.164871200819453\n",
      "348-th epoch val loss 1.2318092950854855\n",
      "349-th epoch train loss 1.1649614207918382\n",
      "349-th epoch val loss 1.231881570765773\n",
      "350-th epoch train loss 1.1650512554212744\n",
      "350-th epoch val loss 1.2319537364196962\n",
      "351-th epoch train loss 1.165140678668668\n",
      "351-th epoch val loss 1.2320257618831043\n",
      "352-th epoch train loss 1.1652296659026575\n",
      "352-th epoch val loss 1.2320976184582304\n",
      "353-th epoch train loss 1.1653181938464108\n",
      "353-th epoch val loss 1.2321692788597536\n",
      "354-th epoch train loss 1.165406240526227\n",
      "354-th epoch val loss 1.2322407171626801\n",
      "355-th epoch train loss 1.1654937852218843\n",
      "355-th epoch val loss 1.2323119087519674\n",
      "356-th epoch train loss 1.1655808084186743\n",
      "356-th epoch val loss 1.2323828302738467\n",
      "357-th epoch train loss 1.1656672917610722\n",
      "357-th epoch val loss 1.232453459588785\n",
      "358-th epoch train loss 1.165753218007987\n",
      "358-th epoch val loss 1.2325237757260312\n",
      "359-th epoch train loss 1.1658385709895391\n",
      "359-th epoch val loss 1.2325937588397002\n",
      "360-th epoch train loss 1.1659233355653214\n",
      "360-th epoch val loss 1.2326633901663424\n",
      "361-th epoch train loss 1.1660074975840868\n",
      "361-th epoch val loss 1.232732651983947\n",
      "362-th epoch train loss 1.1660910438448253\n",
      "362-th epoch val loss 1.2328015275723403\n",
      "363-th epoch train loss 1.1661739620591762\n",
      "363-th epoch val loss 1.2328700011749296\n",
      "364-th epoch train loss 1.16625624081514\n",
      "364-th epoch val loss 1.2329380579617453\n",
      "365-th epoch train loss 1.166337869542041\n",
      "365-th epoch val loss 1.2330056839937473\n",
      "366-th epoch train loss 1.1664188384767051\n",
      "366-th epoch val loss 1.2330728661883488\n",
      "367-th epoch train loss 1.1664991386308066\n",
      "367-th epoch val loss 1.2331395922861164\n",
      "368-th epoch train loss 1.1665787617593542\n",
      "368-th epoch val loss 1.2332058508186172\n",
      "369-th epoch train loss 1.1666577003302703\n",
      "369-th epoch val loss 1.2332716310773617\n",
      "370-th epoch train loss 1.1667359474950347\n",
      "370-th epoch val loss 1.2333369230838227\n",
      "371-th epoch train loss 1.166813497060353\n",
      "371-th epoch val loss 1.233401717560481\n",
      "372-th epoch train loss 1.1668903434608162\n",
      "372-th epoch val loss 1.2334660059028733\n",
      "373-th epoch train loss 1.1669664817325258\n",
      "373-th epoch val loss 1.2335297801526035\n",
      "374-th epoch train loss 1.167041907487642\n",
      "374-th epoch val loss 1.2335930329712939\n",
      "375-th epoch train loss 1.1671166168898308\n",
      "375-th epoch val loss 1.233655757615437\n",
      "376-th epoch train loss 1.1671906066305848\n",
      "376-th epoch val loss 1.233717947912123\n",
      "377-th epoch train loss 1.167263873906373\n",
      "377-th epoch val loss 1.2337795982356128\n",
      "378-th epoch train loss 1.167336416396614\n",
      "378-th epoch val loss 1.2338407034847287\n",
      "379-th epoch train loss 1.1674082322424262\n",
      "379-th epoch val loss 1.2339012590610408\n",
      "380-th epoch train loss 1.1674793200261406\n",
      "380-th epoch val loss 1.2339612608478132\n",
      "381-th epoch train loss 1.1675496787515467\n",
      "381-th epoch val loss 1.234020705189692\n",
      "382-th epoch train loss 1.1676193078248487\n",
      "382-th epoch val loss 1.2340795888731126\n",
      "383-th epoch train loss 1.167688207036305\n",
      "383-th epoch val loss 1.2341379091073938\n",
      "384-th epoch train loss 1.167756376542534\n",
      "384-th epoch val loss 1.2341956635065046\n",
      "385-th epoch train loss 1.1678238168494597\n",
      "385-th epoch val loss 1.2342528500714804\n",
      "386-th epoch train loss 1.1678905287958758\n",
      "386-th epoch val loss 1.2343094671734571\n",
      "387-th epoch train loss 1.1679565135376115\n",
      "387-th epoch val loss 1.2343655135373208\n",
      "388-th epoch train loss 1.1680217725322715\n",
      "388-th epoch val loss 1.234420988225933\n",
      "389-th epoch train loss 1.1680863075245431\n",
      "389-th epoch val loss 1.2344758906249298\n",
      "390-th epoch train loss 1.1681501205320355\n",
      "390-th epoch val loss 1.2345302204280606\n",
      "391-th epoch train loss 1.1682132138316486\n",
      "391-th epoch val loss 1.2345839776230614\n",
      "392-th epoch train loss 1.1682755899464434\n",
      "392-th epoch val loss 1.2346371624780346\n",
      "393-th epoch train loss 1.1683372516330013\n",
      "393-th epoch val loss 1.234689775528328\n",
      "394-th epoch train loss 1.1683982018692562\n",
      "394-th epoch val loss 1.234741817563886\n",
      "395-th epoch train loss 1.168458443842779\n",
      "395-th epoch val loss 1.234793289617065\n",
      "396-th epoch train loss 1.1685179809395083\n",
      "396-th epoch val loss 1.2348441929508986\n",
      "397-th epoch train loss 1.1685768167329003\n",
      "397-th epoch val loss 1.2348945290477837\n",
      "398-th epoch train loss 1.1686349549734938\n",
      "398-th epoch val loss 1.2349442995985969\n",
      "399-th epoch train loss 1.1686923995788716\n",
      "399-th epoch val loss 1.234993506492203\n",
      "400-th epoch train loss 1.1687491546240034\n",
      "400-th epoch val loss 1.2350421518053576\n",
      "401-th epoch train loss 1.1688052243319624\n",
      "401-th epoch val loss 1.2350902377929858\n",
      "402-th epoch train loss 1.1688606130649977\n",
      "402-th epoch val loss 1.2351377668788226\n",
      "403-th epoch train loss 1.1689153253159532\n",
      "403-th epoch val loss 1.2351847416464066\n",
      "404-th epoch train loss 1.1689693657000197\n",
      "404-th epoch val loss 1.2352311648304106\n",
      "405-th epoch train loss 1.1690227389468109\n",
      "405-th epoch val loss 1.2352770393083015\n",
      "406-th epoch train loss 1.1690754498927478\n",
      "406-th epoch val loss 1.235322368092318\n",
      "407-th epoch train loss 1.1691275034737478\n",
      "407-th epoch val loss 1.2353671543217535\n",
      "408-th epoch train loss 1.1691789047182004\n",
      "408-th epoch val loss 1.2354114012555302\n",
      "409-th epoch train loss 1.1692296587402258\n",
      "409-th epoch val loss 1.2354551122650679\n",
      "410-th epoch train loss 1.1692797707332026\n",
      "410-th epoch val loss 1.2354982908274206\n",
      "411-th epoch train loss 1.1693292459635567\n",
      "411-th epoch val loss 1.2355409405186812\n",
      "412-th epoch train loss 1.169378089764803\n",
      "412-th epoch val loss 1.2355830650076474\n",
      "413-th epoch train loss 1.1694263075318299\n",
      "413-th epoch val loss 1.235624668049729\n",
      "414-th epoch train loss 1.1694739047154201\n",
      "414-th epoch val loss 1.2356657534811015\n",
      "415-th epoch train loss 1.169520886816996\n",
      "415-th epoch val loss 1.2357063252130895\n",
      "416-th epoch train loss 1.1695672593835835\n",
      "416-th epoch val loss 1.23574638722677\n",
      "417-th epoch train loss 1.1696130280029904\n",
      "417-th epoch val loss 1.2357859435677978\n",
      "418-th epoch train loss 1.1696581982991854\n",
      "418-th epoch val loss 1.2358249983414338\n",
      "419-th epoch train loss 1.169702775927872\n",
      "419-th epoch val loss 1.2358635557077755\n",
      "420-th epoch train loss 1.1697467665722585\n",
      "420-th epoch val loss 1.2359016198771882\n",
      "421-th epoch train loss 1.1697901759390017\n",
      "421-th epoch val loss 1.2359391951059113\n",
      "422-th epoch train loss 1.1698330097543341\n",
      "422-th epoch val loss 1.2359762856918575\n",
      "423-th epoch train loss 1.1698752737603604\n",
      "423-th epoch val loss 1.2360128959705803\n",
      "424-th epoch train loss 1.1699169737115125\n",
      "424-th epoch val loss 1.2360490303114102\n",
      "425-th epoch train loss 1.1699581153711682\n",
      "425-th epoch val loss 1.2360846931137541\n",
      "426-th epoch train loss 1.1699987045084206\n",
      "426-th epoch val loss 1.2361198888035543\n",
      "427-th epoch train loss 1.1700387468949935\n",
      "427-th epoch val loss 1.2361546218298916\n",
      "428-th epoch train loss 1.1700782483023004\n",
      "428-th epoch val loss 1.236188896661745\n",
      "429-th epoch train loss 1.1701172144986371\n",
      "429-th epoch val loss 1.23622271778488\n",
      "430-th epoch train loss 1.1701556512465088\n",
      "430-th epoch val loss 1.2362560896988826\n",
      "431-th epoch train loss 1.1701935643000811\n",
      "431-th epoch val loss 1.2362890169143184\n",
      "432-th epoch train loss 1.1702309594027585\n",
      "432-th epoch val loss 1.2363215039500206\n",
      "433-th epoch train loss 1.1702678422848714\n",
      "433-th epoch val loss 1.2363535553304976\n",
      "434-th epoch train loss 1.1703042186614858\n",
      "434-th epoch val loss 1.2363851755834605\n",
      "435-th epoch train loss 1.1703400942303162\n",
      "435-th epoch val loss 1.2364163692374621\n",
      "436-th epoch train loss 1.1703754746697475\n",
      "436-th epoch val loss 1.2364471408196438\n",
      "437-th epoch train loss 1.1704103656369533\n",
      "437-th epoch val loss 1.2364774948535897\n",
      "438-th epoch train loss 1.1704447727661167\n",
      "438-th epoch val loss 1.2365074358572818\n",
      "439-th epoch train loss 1.1704787016667422\n",
      "439-th epoch val loss 1.2365369683411513\n",
      "440-th epoch train loss 1.1705121579220576\n",
      "440-th epoch val loss 1.2365660968062222\n",
      "441-th epoch train loss 1.1705451470875041\n",
      "441-th epoch val loss 1.2365948257423534\n",
      "442-th epoch train loss 1.1705776746893106\n",
      "442-th epoch val loss 1.2366231596265571\n",
      "443-th epoch train loss 1.1706097462231442\n",
      "443-th epoch val loss 1.2366511029214082\n",
      "444-th epoch train loss 1.1706413671528437\n",
      "444-th epoch val loss 1.2366786600735333\n",
      "445-th epoch train loss 1.1706725429092237\n",
      "445-th epoch val loss 1.2367058355121763\n",
      "446-th epoch train loss 1.170703278888952\n",
      "446-th epoch val loss 1.236732633647841\n",
      "447-th epoch train loss 1.1707335804534926\n",
      "447-th epoch val loss 1.2367590588710007\n",
      "448-th epoch train loss 1.1707634529281217\n",
      "448-th epoch val loss 1.2367851155508873\n",
      "449-th epoch train loss 1.1707929016009986\n",
      "449-th epoch val loss 1.2368108080343339\n",
      "450-th epoch train loss 1.1708219317223032\n",
      "450-th epoch val loss 1.236836140644694\n",
      "451-th epoch train loss 1.1708505485034308\n",
      "451-th epoch val loss 1.2368611176808142\n",
      "452-th epoch train loss 1.1708787571162433\n",
      "452-th epoch val loss 1.2368857434160696\n",
      "453-th epoch train loss 1.1709065626923731\n",
      "453-th epoch val loss 1.236910022097458\n",
      "454-th epoch train loss 1.1709339703225803\n",
      "454-th epoch val loss 1.2369339579447447\n",
      "455-th epoch train loss 1.17096098505616\n",
      "455-th epoch val loss 1.236957555149664\n",
      "456-th epoch train loss 1.1709876119003952\n",
      "456-th epoch val loss 1.2369808178751684\n",
      "457-th epoch train loss 1.1710138558200585\n",
      "457-th epoch val loss 1.2370037502547315\n",
      "458-th epoch train loss 1.171039721736955\n",
      "458-th epoch val loss 1.237026356391689\n",
      "459-th epoch train loss 1.1710652145295095\n",
      "459-th epoch val loss 1.2370486403586374\n",
      "460-th epoch train loss 1.1710903390323915\n",
      "460-th epoch val loss 1.237070606196861\n",
      "461-th epoch train loss 1.1711151000361821\n",
      "461-th epoch val loss 1.2370922579158135\n",
      "462-th epoch train loss 1.1711395022870756\n",
      "462-th epoch val loss 1.2371135994926323\n",
      "463-th epoch train loss 1.1711635504866178\n",
      "463-th epoch val loss 1.237134634871692\n",
      "464-th epoch train loss 1.1711872492914768\n",
      "464-th epoch val loss 1.2371553679641962\n",
      "465-th epoch train loss 1.1712106033132477\n",
      "465-th epoch val loss 1.2371758026478046\n",
      "466-th epoch train loss 1.171233617118289\n",
      "466-th epoch val loss 1.2371959427662893\n",
      "467-th epoch train loss 1.171256295227586\n",
      "467-th epoch val loss 1.2372157921292304\n",
      "468-th epoch train loss 1.171278642116646\n",
      "468-th epoch val loss 1.2372353545117378\n",
      "469-th epoch train loss 1.1713006622154163\n",
      "469-th epoch val loss 1.2372546336542039\n",
      "470-th epoch train loss 1.1713223599082356\n",
      "470-th epoch val loss 1.2372736332620842\n",
      "471-th epoch train loss 1.1713437395338002\n",
      "471-th epoch val loss 1.2372923570057077\n",
      "472-th epoch train loss 1.171364805385162\n",
      "472-th epoch val loss 1.237310808520109\n",
      "473-th epoch train loss 1.1713855617097448\n",
      "473-th epoch val loss 1.2373289914048884\n",
      "474-th epoch train loss 1.1714060127093846\n",
      "474-th epoch val loss 1.2373469092240963\n",
      "475-th epoch train loss 1.171426162540388\n",
      "475-th epoch val loss 1.237364565506137\n",
      "476-th epoch train loss 1.171446015313611\n",
      "476-th epoch val loss 1.2373819637436987\n",
      "477-th epoch train loss 1.1714655750945586\n",
      "477-th epoch val loss 1.237399107393701\n",
      "478-th epoch train loss 1.1714848459034966\n",
      "478-th epoch val loss 1.2374159998772623\n",
      "479-th epoch train loss 1.1715038317155868\n",
      "479-th epoch val loss 1.2374326445796895\n",
      "480-th epoch train loss 1.1715225364610324\n",
      "480-th epoch val loss 1.2374490448504814\n",
      "481-th epoch train loss 1.1715409640252428\n",
      "481-th epoch val loss 1.237465204003352\n",
      "482-th epoch train loss 1.1715591182490097\n",
      "482-th epoch val loss 1.23748112531627\n",
      "483-th epoch train loss 1.171577002928698\n",
      "483-th epoch val loss 1.237496812031513\n",
      "484-th epoch train loss 1.171594621816452\n",
      "484-th epoch val loss 1.2375122673557386\n",
      "485-th epoch train loss 1.1716119786204062\n",
      "485-th epoch val loss 1.237527494460066\n",
      "486-th epoch train loss 1.17162907700492\n",
      "486-th epoch val loss 1.2375424964801762\n",
      "487-th epoch train loss 1.1716459205908076\n",
      "487-th epoch val loss 1.2375572765164178\n",
      "488-th epoch train loss 1.171662512955593\n",
      "488-th epoch val loss 1.2375718376339346\n",
      "489-th epoch train loss 1.171678857633766\n",
      "489-th epoch val loss 1.2375861828627954\n",
      "490-th epoch train loss 1.1716949581170495\n",
      "490-th epoch val loss 1.2376003151981412\n",
      "491-th epoch train loss 1.171710817854674\n",
      "491-th epoch val loss 1.237614237600338\n",
      "492-th epoch train loss 1.171726440253663\n",
      "492-th epoch val loss 1.2376279529951448\n",
      "493-th epoch train loss 1.1717418286791232\n",
      "493-th epoch val loss 1.237641464273885\n",
      "494-th epoch train loss 1.1717569864545447\n",
      "494-th epoch val loss 1.2376547742936301\n",
      "495-th epoch train loss 1.1717719168621015\n",
      "495-th epoch val loss 1.2376678858773928\n",
      "496-th epoch train loss 1.1717866231429666\n",
      "496-th epoch val loss 1.2376808018143228\n",
      "497-th epoch train loss 1.1718011084976263\n",
      "497-th epoch val loss 1.237693524859916\n",
      "498-th epoch train loss 1.1718153760862022\n",
      "498-th epoch val loss 1.2377060577362262\n",
      "499-th epoch train loss 1.1718294290287774\n",
      "499-th epoch val loss 1.2377184031320851\n",
      "500-th epoch train loss 1.171843270405727\n",
      "500-th epoch val loss 1.2377305637033278\n",
      "501-th epoch train loss 1.1718569032580546\n",
      "501-th epoch val loss 1.2377425420730244\n",
      "502-th epoch train loss 1.1718703305877296\n",
      "502-th epoch val loss 1.2377543408317166\n",
      "503-th epoch train loss 1.1718835553580296\n",
      "503-th epoch val loss 1.2377659625376598\n",
      "504-th epoch train loss 1.1718965804938855\n",
      "504-th epoch val loss 1.2377774097170695\n",
      "505-th epoch train loss 1.171909408882229\n",
      "505-th epoch val loss 1.237788684864368\n",
      "506-th epoch train loss 1.1719220433723436\n",
      "506-th epoch val loss 1.2377997904424454\n",
      "507-th epoch train loss 1.171934486776216\n",
      "507-th epoch val loss 1.2378107288829119\n",
      "508-th epoch train loss 1.1719467418688927\n",
      "508-th epoch val loss 1.2378215025863624\n",
      "509-th epoch train loss 1.1719588113888333\n",
      "509-th epoch val loss 1.237832113922639\n",
      "510-th epoch train loss 1.1719706980382678\n",
      "510-th epoch val loss 1.2378425652310983\n",
      "511-th epoch train loss 1.1719824044835587\n",
      "511-th epoch val loss 1.2378528588208826\n",
      "512-th epoch train loss 1.171993933355556\n",
      "512-th epoch val loss 1.2378629969711905\n",
      "513-th epoch train loss 1.1720052872499604\n",
      "513-th epoch val loss 1.2378729819315506\n",
      "514-th epoch train loss 1.1720164687276817\n",
      "514-th epoch val loss 1.2378828159220971\n",
      "515-th epoch train loss 1.172027480315202\n",
      "515-th epoch val loss 1.2378925011338497\n",
      "516-th epoch train loss 1.1720383245049335\n",
      "516-th epoch val loss 1.2379020397289888\n",
      "517-th epoch train loss 1.1720490037555837\n",
      "517-th epoch val loss 1.237911433841138\n",
      "518-th epoch train loss 1.172059520492512\n",
      "518-th epoch val loss 1.2379206855756422\n",
      "519-th epoch train loss 1.172069877108091\n",
      "519-th epoch val loss 1.2379297970098524\n",
      "520-th epoch train loss 1.1720800759620695\n",
      "520-th epoch val loss 1.2379387701934073\n",
      "521-th epoch train loss 1.1720901193819255\n",
      "521-th epoch val loss 1.2379476071485132\n",
      "522-th epoch train loss 1.1721000096632295\n",
      "522-th epoch val loss 1.237956309870231\n",
      "523-th epoch train loss 1.1721097490699983\n",
      "523-th epoch val loss 1.237964880326756\n",
      "524-th epoch train loss 1.1721193398350527\n",
      "524-th epoch val loss 1.237973320459704\n",
      "525-th epoch train loss 1.1721287841603711\n",
      "525-th epoch val loss 1.2379816321843924\n",
      "526-th epoch train loss 1.1721380842174443\n",
      "526-th epoch val loss 1.2379898173901245\n",
      "527-th epoch train loss 1.1721472421476247\n",
      "527-th epoch val loss 1.2379978779404697\n",
      "528-th epoch train loss 1.17215626006248\n",
      "528-th epoch val loss 1.2380058156735474\n",
      "529-th epoch train loss 1.1721651400441386\n",
      "529-th epoch val loss 1.238013632402308\n",
      "530-th epoch train loss 1.1721738841456382\n",
      "530-th epoch val loss 1.2380213299148124\n",
      "531-th epoch train loss 1.1721824943912706\n",
      "531-th epoch val loss 1.2380289099745125\n",
      "532-th epoch train loss 1.1721909727769242\n",
      "532-th epoch val loss 1.2380363743205303\n",
      "533-th epoch train loss 1.1721993212704254\n",
      "533-th epoch val loss 1.2380437246679346\n",
      "534-th epoch train loss 1.1722075418118767\n",
      "534-th epoch val loss 1.2380509627080172\n",
      "535-th epoch train loss 1.1722156363139955\n",
      "535-th epoch val loss 1.2380580901085718\n",
      "536-th epoch train loss 1.1722236066624456\n",
      "536-th epoch val loss 1.238065108514163\n",
      "537-th epoch train loss 1.1722314547161725\n",
      "537-th epoch val loss 1.2380720195464028\n",
      "538-th epoch train loss 1.1722391823077303\n",
      "538-th epoch val loss 1.2380788248042205\n",
      "539-th epoch train loss 1.1722467912436128\n",
      "539-th epoch val loss 1.2380855258641335\n",
      "540-th epoch train loss 1.172254283304575\n",
      "540-th epoch val loss 1.2380921242805139\n",
      "541-th epoch train loss 1.1722616602459572\n",
      "541-th epoch val loss 1.2380986215858556\n",
      "542-th epoch train loss 1.1722689237980062\n",
      "542-th epoch val loss 1.238105019291043\n",
      "543-th epoch train loss 1.172276075666191\n",
      "543-th epoch val loss 1.2381113188856074\n",
      "544-th epoch train loss 1.1722831175315176\n",
      "544-th epoch val loss 1.2381175218379954\n",
      "545-th epoch train loss 1.1722900510508436\n",
      "545-th epoch val loss 1.2381236295958238\n",
      "546-th epoch train loss 1.1722968778571845\n",
      "546-th epoch val loss 1.23812964358614\n",
      "547-th epoch train loss 1.172303599560023\n",
      "547-th epoch val loss 1.2381355652156774\n",
      "548-th epoch train loss 1.1723102177456128\n",
      "548-th epoch val loss 1.2381413958711083\n",
      "549-th epoch train loss 1.1723167339772786\n",
      "549-th epoch val loss 1.2381471369192985\n",
      "550-th epoch train loss 1.172323149795715\n",
      "550-th epoch val loss 1.238152789707553\n",
      "551-th epoch train loss 1.1723294667192858\n",
      "551-th epoch val loss 1.2381583555638693\n",
      "552-th epoch train loss 1.1723356862443108\n",
      "552-th epoch val loss 1.23816383579718\n",
      "553-th epoch train loss 1.1723418098453626\n",
      "553-th epoch val loss 1.2381692316975974\n",
      "554-th epoch train loss 1.1723478389755486\n",
      "554-th epoch val loss 1.2381745445366543\n",
      "555-th epoch train loss 1.1723537750667998\n",
      "555-th epoch val loss 1.2381797755675472\n",
      "556-th epoch train loss 1.1723596195301493\n",
      "556-th epoch val loss 1.238184926025369\n",
      "557-th epoch train loss 1.1723653737560131\n",
      "557-th epoch val loss 1.2381899971273478\n",
      "558-th epoch train loss 1.172371039114466\n",
      "558-th epoch val loss 1.2381949900730784\n",
      "559-th epoch train loss 1.172376616955514\n",
      "559-th epoch val loss 1.2381999060447546\n",
      "560-th epoch train loss 1.1723821086093653\n",
      "560-th epoch val loss 1.2382047462073955\n",
      "561-th epoch train loss 1.1723875153866976\n",
      "561-th epoch val loss 1.2382095117090752\n",
      "562-th epoch train loss 1.1723928385789233\n",
      "562-th epoch val loss 1.238214203681146\n",
      "563-th epoch train loss 1.172398079458449\n",
      "563-th epoch val loss 1.2382188232384586\n",
      "564-th epoch train loss 1.1724032392789385\n",
      "564-th epoch val loss 1.238223371479585\n",
      "565-th epoch train loss 1.172408319275565\n",
      "565-th epoch val loss 1.2382278494870336\n",
      "566-th epoch train loss 1.1724133206652665\n",
      "566-th epoch val loss 1.238232258327467\n",
      "567-th epoch train loss 1.172418244646995\n",
      "567-th epoch val loss 1.2382365990519117\n",
      "568-th epoch train loss 1.172423092401966\n",
      "568-th epoch val loss 1.2382408726959748\n",
      "569-th epoch train loss 1.1724278650939013\n",
      "569-th epoch val loss 1.2382450802800455\n",
      "570-th epoch train loss 1.1724325638692727\n",
      "570-th epoch val loss 1.2382492228095083\n",
      "571-th epoch train loss 1.172437189857539\n",
      "571-th epoch val loss 1.2382533012749424\n",
      "572-th epoch train loss 1.1724417441713848\n",
      "572-th epoch val loss 1.238257316652324\n",
      "573-th epoch train loss 1.172446227906953\n",
      "573-th epoch val loss 1.2382612699032314\n",
      "574-th epoch train loss 1.172450642144077\n",
      "574-th epoch val loss 1.2382651619750356\n",
      "575-th epoch train loss 1.1724549879465056\n",
      "575-th epoch val loss 1.2382689938010991\n",
      "576-th epoch train loss 1.1724592663621334\n",
      "576-th epoch val loss 1.2382727663009707\n",
      "577-th epoch train loss 1.172463478423219\n",
      "577-th epoch val loss 1.238276480380571\n",
      "578-th epoch train loss 1.1724676251466086\n",
      "578-th epoch val loss 1.2382801369323877\n",
      "579-th epoch train loss 1.1724717075339508\n",
      "579-th epoch val loss 1.2382837368356563\n",
      "580-th epoch train loss 1.1724757265719137\n",
      "580-th epoch val loss 1.2382872809565495\n",
      "581-th epoch train loss 1.1724796832323952\n",
      "581-th epoch val loss 1.2382907701483548\n",
      "582-th epoch train loss 1.172483578472733\n",
      "582-th epoch val loss 1.2382942052516586\n",
      "583-th epoch train loss 1.1724874132359124\n",
      "583-th epoch val loss 1.2382975870945216\n",
      "584-th epoch train loss 1.1724911884507703\n",
      "584-th epoch val loss 1.2383009164926555\n",
      "585-th epoch train loss 1.172494905032196\n",
      "585-th epoch val loss 1.2383041942495963\n",
      "586-th epoch train loss 1.1724985638813314\n",
      "586-th epoch val loss 1.2383074211568763\n",
      "587-th epoch train loss 1.1725021658857688\n",
      "587-th epoch val loss 1.2383105979941942\n",
      "588-th epoch train loss 1.1725057119197424\n",
      "588-th epoch val loss 1.2383137255295802\n",
      "589-th epoch train loss 1.1725092028443225\n",
      "589-th epoch val loss 1.238316804519565\n",
      "590-th epoch train loss 1.1725126395076042\n",
      "590-th epoch val loss 1.2383198357093397\n",
      "591-th epoch train loss 1.1725160227448939\n",
      "591-th epoch val loss 1.2383228198329193\n",
      "592-th epoch train loss 1.1725193533788931\n",
      "592-th epoch val loss 1.2383257576133004\n",
      "593-th epoch train loss 1.1725226322198832\n",
      "593-th epoch val loss 1.2383286497626207\n",
      "594-th epoch train loss 1.1725258600659019\n",
      "594-th epoch val loss 1.2383314969823118\n",
      "595-th epoch train loss 1.172529037702923\n",
      "595-th epoch val loss 1.2383342999632536\n",
      "596-th epoch train loss 1.1725321659050298\n",
      "596-th epoch val loss 1.2383370593859269\n",
      "597-th epoch train loss 1.1725352454345894\n",
      "597-th epoch val loss 1.2383397759205608\n",
      "598-th epoch train loss 1.1725382770424213\n",
      "598-th epoch val loss 1.2383424502272806\n",
      "599-th epoch train loss 1.1725412614679682\n",
      "599-th epoch val loss 1.238345082956256\n",
      "600-th epoch train loss 1.172544199439459\n",
      "600-th epoch val loss 1.2383476747478415\n",
      "601-th epoch train loss 1.1725470916740748\n",
      "601-th epoch val loss 1.23835022623272\n",
      "602-th epoch train loss 1.1725499388781089\n",
      "602-th epoch val loss 1.2383527380320418\n",
      "603-th epoch train loss 1.1725527417471282\n",
      "603-th epoch val loss 1.238355210757565\n",
      "604-th epoch train loss 1.1725555009661295\n",
      "604-th epoch val loss 1.23835764501179\n",
      "605-th epoch train loss 1.1725582172096924\n",
      "605-th epoch val loss 1.2383600413880933\n",
      "606-th epoch train loss 1.1725608911421364\n",
      "606-th epoch val loss 1.238362400470863\n",
      "607-th epoch train loss 1.1725635234176675\n",
      "607-th epoch val loss 1.238364722835627\n",
      "608-th epoch train loss 1.1725661146805322\n",
      "608-th epoch val loss 1.2383670090491856\n",
      "609-th epoch train loss 1.1725686655651602\n",
      "609-th epoch val loss 1.238369259669736\n",
      "610-th epoch train loss 1.1725711766963096\n",
      "610-th epoch val loss 1.238371475246999\n",
      "611-th epoch train loss 1.1725736486892129\n",
      "611-th epoch val loss 1.2383736563223449\n",
      "612-th epoch train loss 1.1725760821497149\n",
      "612-th epoch val loss 1.238375803428913\n",
      "613-th epoch train loss 1.172578477674412\n",
      "613-th epoch val loss 1.2383779170917348\n",
      "614-th epoch train loss 1.1725808358507899\n",
      "614-th epoch val loss 1.238379997827854\n",
      "615-th epoch train loss 1.17258315725736\n",
      "615-th epoch val loss 1.2383820461464405\n",
      "616-th epoch train loss 1.1725854424637898\n",
      "616-th epoch val loss 1.2383840625489095\n",
      "617-th epoch train loss 1.172587692031037\n",
      "617-th epoch val loss 1.238386047529035\n",
      "618-th epoch train loss 1.1725899065114769\n",
      "618-th epoch val loss 1.238388001573062\n",
      "619-th epoch train loss 1.1725920864490325\n",
      "619-th epoch val loss 1.2383899251598187\n",
      "620-th epoch train loss 1.1725942323792982\n",
      "620-th epoch val loss 1.2383918187608265\n",
      "621-th epoch train loss 1.1725963448296646\n",
      "621-th epoch val loss 1.2383936828404067\n",
      "622-th epoch train loss 1.1725984243194427\n",
      "622-th epoch val loss 1.2383955178557884\n",
      "623-th epoch train loss 1.1726004713599825\n",
      "623-th epoch val loss 1.2383973242572137\n",
      "624-th epoch train loss 1.1726024864547924\n",
      "624-th epoch val loss 1.2383991024880412\n",
      "625-th epoch train loss 1.1726044700996585\n",
      "625-th epoch val loss 1.2384008529848494\n",
      "626-th epoch train loss 1.1726064227827568\n",
      "626-th epoch val loss 1.2384025761775355\n",
      "627-th epoch train loss 1.1726083449847702\n",
      "627-th epoch val loss 1.2384042724894155\n",
      "628-th epoch train loss 1.1726102371790004\n",
      "628-th epoch val loss 1.2384059423373255\n",
      "629-th epoch train loss 1.1726120998314769\n",
      "629-th epoch val loss 1.2384075861317132\n",
      "630-th epoch train loss 1.1726139334010683\n",
      "630-th epoch val loss 1.2384092042767374\n",
      "631-th epoch train loss 1.1726157383395897\n",
      "631-th epoch val loss 1.2384107971703604\n",
      "632-th epoch train loss 1.1726175150919076\n",
      "632-th epoch val loss 1.2384123652044405\n",
      "633-th epoch train loss 1.1726192640960444\n",
      "633-th epoch val loss 1.2384139087648227\n",
      "634-th epoch train loss 1.172620985783284\n",
      "634-th epoch val loss 1.238415428231433\n",
      "635-th epoch train loss 1.1726226805782718\n",
      "635-th epoch val loss 1.2384169239783602\n",
      "636-th epoch train loss 1.172624348899113\n",
      "636-th epoch val loss 1.2384183963739497\n",
      "637-th epoch train loss 1.1726259911574748\n",
      "637-th epoch val loss 1.2384198457808862\n",
      "638-th epoch train loss 1.1726276077586806\n",
      "638-th epoch val loss 1.2384212725562809\n",
      "639-th epoch train loss 1.1726291991018087\n",
      "639-th epoch val loss 1.238422677051753\n",
      "640-th epoch train loss 1.172630765579784\n",
      "640-th epoch val loss 1.2384240596135148\n",
      "641-th epoch train loss 1.1726323075794731\n",
      "641-th epoch val loss 1.238425420582451\n",
      "642-th epoch train loss 1.1726338254817756\n",
      "642-th epoch val loss 1.2384267602942016\n",
      "643-th epoch train loss 1.172635319661713\n",
      "643-th epoch val loss 1.2384280790792372\n",
      "644-th epoch train loss 1.1726367904885204\n",
      "644-th epoch val loss 1.2384293772629411\n",
      "645-th epoch train loss 1.1726382383257328\n",
      "645-th epoch val loss 1.2384306551656834\n",
      "646-th epoch train loss 1.1726396635312715\n",
      "646-th epoch val loss 1.2384319131028976\n",
      "647-th epoch train loss 1.1726410664575289\n",
      "647-th epoch val loss 1.2384331513851548\n",
      "648-th epoch train loss 1.1726424474514545\n",
      "648-th epoch val loss 1.2384343703182386\n",
      "649-th epoch train loss 1.1726438068546357\n",
      "649-th epoch val loss 1.2384355702032164\n",
      "650-th epoch train loss 1.17264514500338\n",
      "650-th epoch val loss 1.2384367513365102\n",
      "651-th epoch train loss 1.1726464622287953\n",
      "651-th epoch val loss 1.2384379140099693\n",
      "652-th epoch train loss 1.1726477588568698\n",
      "652-th epoch val loss 1.2384390585109373\n",
      "653-th epoch train loss 1.172649035208549\n",
      "653-th epoch val loss 1.2384401851223217\n",
      "654-th epoch train loss 1.1726502915998118\n",
      "654-th epoch val loss 1.2384412941226601\n",
      "655-th epoch train loss 1.1726515283417496\n",
      "655-th epoch val loss 1.2384423857861901\n",
      "656-th epoch train loss 1.172652745740636\n",
      "656-th epoch val loss 1.2384434603829082\n",
      "657-th epoch train loss 1.172653944098005\n",
      "657-th epoch val loss 1.2384445181786414\n",
      "658-th epoch train loss 1.17265512371072\n",
      "658-th epoch val loss 1.2384455594351056\n",
      "659-th epoch train loss 1.1726562848710471\n",
      "659-th epoch val loss 1.23844658440997\n",
      "660-th epoch train loss 1.1726574278667241\n",
      "660-th epoch val loss 1.238447593356919\n",
      "661-th epoch train loss 1.172658552981031\n",
      "661-th epoch val loss 1.2384485865257122\n",
      "662-th epoch train loss 1.1726596604928565\n",
      "662-th epoch val loss 1.2384495641622453\n",
      "663-th epoch train loss 1.1726607506767674\n",
      "663-th epoch val loss 1.2384505265086077\n",
      "664-th epoch train loss 1.1726618238030728\n",
      "664-th epoch val loss 1.238451473803141\n",
      "665-th epoch train loss 1.17266288013789\n",
      "665-th epoch val loss 1.2384524062804965\n",
      "666-th epoch train loss 1.1726639199432094\n",
      "666-th epoch val loss 1.2384533241716926\n",
      "667-th epoch train loss 1.1726649434769565\n",
      "667-th epoch val loss 1.2384542277041677\n",
      "668-th epoch train loss 1.1726659509930555\n",
      "668-th epoch val loss 1.2384551171018374\n",
      "669-th epoch train loss 1.1726669427414886\n",
      "669-th epoch val loss 1.2384559925851457\n",
      "670-th epoch train loss 1.1726679189683602\n",
      "670-th epoch val loss 1.238456854371122\n",
      "671-th epoch train loss 1.172668879915952\n",
      "671-th epoch val loss 1.2384577026734302\n",
      "672-th epoch train loss 1.1726698258227848\n",
      "672-th epoch val loss 1.2384585377024206\n",
      "673-th epoch train loss 1.1726707569236754\n",
      "673-th epoch val loss 1.2384593596651814\n",
      "674-th epoch train loss 1.1726716734497928\n",
      "674-th epoch val loss 1.2384601687655896\n",
      "675-th epoch train loss 1.172672575628716\n",
      "675-th epoch val loss 1.238460965204358\n",
      "676-th epoch train loss 1.1726734636844867\n",
      "676-th epoch val loss 1.2384617491790855\n",
      "677-th epoch train loss 1.1726743378376656\n",
      "677-th epoch val loss 1.2384625208843032\n",
      "678-th epoch train loss 1.172675198305387\n",
      "678-th epoch val loss 1.2384632805115243\n",
      "679-th epoch train loss 1.1726760453014067\n",
      "679-th epoch val loss 1.238464028249287\n",
      "680-th epoch train loss 1.1726768790361601\n",
      "680-th epoch val loss 1.2384647642832023\n",
      "681-th epoch train loss 1.1726776997168102\n",
      "681-th epoch val loss 1.2384654887959992\n",
      "682-th epoch train loss 1.172678507547297\n",
      "682-th epoch val loss 1.2384662019675674\n",
      "683-th epoch train loss 1.172679302728389\n",
      "683-th epoch val loss 1.2384669039750023\n",
      "684-th epoch train loss 1.172680085457733\n",
      "684-th epoch val loss 1.2384675949926476\n",
      "685-th epoch train loss 1.172680855929899\n",
      "685-th epoch val loss 1.2384682751921372\n",
      "686-th epoch train loss 1.1726816143364303\n",
      "686-th epoch val loss 1.2384689447424373\n",
      "687-th epoch train loss 1.1726823608658896\n",
      "687-th epoch val loss 1.2384696038098886\n",
      "688-th epoch train loss 1.1726830957039043\n",
      "688-th epoch val loss 1.2384702525582432\n",
      "689-th epoch train loss 1.1726838190332118\n",
      "689-th epoch val loss 1.2384708911487086\n",
      "690-th epoch train loss 1.1726845310337064\n",
      "690-th epoch val loss 1.238471519739985\n",
      "691-th epoch train loss 1.1726852318824796\n",
      "691-th epoch val loss 1.238472138488303\n",
      "692-th epoch train loss 1.1726859217538665\n",
      "692-th epoch val loss 1.2384727475474644\n",
      "693-th epoch train loss 1.1726866008194856\n",
      "693-th epoch val loss 1.2384733470688765\n",
      "694-th epoch train loss 1.1726872692482841\n",
      "694-th epoch val loss 1.2384739372015912\n",
      "695-th epoch train loss 1.172687927206576\n",
      "695-th epoch val loss 1.2384745180923407\n",
      "696-th epoch train loss 1.1726885748580849\n",
      "696-th epoch val loss 1.2384750898855736\n",
      "697-th epoch train loss 1.1726892123639834\n",
      "697-th epoch val loss 1.2384756527234886\n",
      "698-th epoch train loss 1.1726898398829315\n",
      "698-th epoch val loss 1.2384762067460713\n",
      "699-th epoch train loss 1.1726904575711186\n",
      "699-th epoch val loss 1.2384767520911286\n",
      "700-th epoch train loss 1.1726910655822973\n",
      "700-th epoch val loss 1.2384772888943187\n",
      "701-th epoch train loss 1.1726916640678249\n",
      "701-th epoch val loss 1.23847781728919\n",
      "702-th epoch train loss 1.1726922531766988\n",
      "702-th epoch val loss 1.2384783374072077\n",
      "703-th epoch train loss 1.172692833055593\n",
      "703-th epoch val loss 1.2384788493777912\n",
      "704-th epoch train loss 1.1726934038488952\n",
      "704-th epoch val loss 1.238479353328343\n",
      "705-th epoch train loss 1.1726939656987403\n",
      "705-th epoch val loss 1.2384798493842792\n",
      "706-th epoch train loss 1.1726945187450477\n",
      "706-th epoch val loss 1.2384803376690627\n",
      "707-th epoch train loss 1.1726950631255528\n",
      "707-th epoch val loss 1.2384808183042317\n",
      "708-th epoch train loss 1.1726955989758439\n",
      "708-th epoch val loss 1.2384812914094296\n",
      "709-th epoch train loss 1.1726961264293936\n",
      "709-th epoch val loss 1.2384817571024356\n",
      "710-th epoch train loss 1.1726966456175918\n",
      "710-th epoch val loss 1.2384822154991915\n",
      "711-th epoch train loss 1.1726971566697784\n",
      "711-th epoch val loss 1.238482666713832\n",
      "712-th epoch train loss 1.172697659713275\n",
      "712-th epoch val loss 1.2384831108587124\n",
      "713-th epoch train loss 1.1726981548734166\n",
      "713-th epoch val loss 1.2384835480444347\n",
      "714-th epoch train loss 1.1726986422735806\n",
      "714-th epoch val loss 1.238483978379877\n",
      "715-th epoch train loss 1.17269912203522\n",
      "715-th epoch val loss 1.2384844019722172\n",
      "716-th epoch train loss 1.172699594277892\n",
      "716-th epoch val loss 1.2384848189269633\n",
      "717-th epoch train loss 1.1727000591192855\n",
      "717-th epoch val loss 1.2384852293479764\n",
      "718-th epoch train loss 1.1727005166752544\n",
      "718-th epoch val loss 1.2384856333374972\n",
      "719-th epoch train loss 1.172700967059842\n",
      "719-th epoch val loss 1.238486030996171\n",
      "720-th epoch train loss 1.1727014103853115\n",
      "720-th epoch val loss 1.2384864224230716\n",
      "721-th epoch train loss 1.1727018467621722\n",
      "721-th epoch val loss 1.2384868077157283\n",
      "722-th epoch train loss 1.1727022762992079\n",
      "722-th epoch val loss 1.2384871869701461\n",
      "723-th epoch train loss 1.1727026991035039\n",
      "723-th epoch val loss 1.2384875602808334\n",
      "724-th epoch train loss 1.1727031152804719\n",
      "724-th epoch val loss 1.2384879277408218\n",
      "725-th epoch train loss 1.1727035249338769\n",
      "725-th epoch val loss 1.2384882894416909\n",
      "726-th epoch train loss 1.1727039281658642\n",
      "726-th epoch val loss 1.2384886454735904\n",
      "727-th epoch train loss 1.1727043250769817\n",
      "727-th epoch val loss 1.2384889959252627\n",
      "728-th epoch train loss 1.172704715766207\n",
      "728-th epoch val loss 1.2384893408840638\n",
      "729-th epoch train loss 1.1727051003309714\n",
      "729-th epoch val loss 1.2384896804359866\n",
      "730-th epoch train loss 1.1727054788671825\n",
      "730-th epoch val loss 1.238490014665679\n",
      "731-th epoch train loss 1.1727058514692499\n",
      "731-th epoch val loss 1.2384903436564678\n",
      "732-th epoch train loss 1.172706218230108\n",
      "732-th epoch val loss 1.2384906674903784\n",
      "733-th epoch train loss 1.1727065792412386\n",
      "733-th epoch val loss 1.238490986248155\n",
      "734-th epoch train loss 1.1727069345926922\n",
      "734-th epoch val loss 1.2384913000092794\n",
      "735-th epoch train loss 1.1727072843731137\n",
      "735-th epoch val loss 1.2384916088519922\n",
      "736-th epoch train loss 1.1727076286697609\n",
      "736-th epoch val loss 1.2384919128533114\n",
      "737-th epoch train loss 1.172707967568528\n",
      "737-th epoch val loss 1.2384922120890518\n",
      "738-th epoch train loss 1.172708301153966\n",
      "738-th epoch val loss 1.2384925066338428\n",
      "739-th epoch train loss 1.1727086295093039\n",
      "739-th epoch val loss 1.2384927965611476\n",
      "740-th epoch train loss 1.1727089527164694\n",
      "740-th epoch val loss 1.2384930819432818\n",
      "741-th epoch train loss 1.172709270856109\n",
      "741-th epoch val loss 1.2384933628514299\n",
      "742-th epoch train loss 1.1727095840076085\n",
      "742-th epoch val loss 1.2384936393556647\n",
      "743-th epoch train loss 1.1727098922491113\n",
      "743-th epoch val loss 1.2384939115249618\n",
      "744-th epoch train loss 1.1727101956575399\n",
      "744-th epoch val loss 1.2384941794272208\n",
      "745-th epoch train loss 1.1727104943086122\n",
      "745-th epoch val loss 1.2384944431292781\n",
      "746-th epoch train loss 1.1727107882768633\n",
      "746-th epoch val loss 1.2384947026969262\n",
      "747-th epoch train loss 1.172711077635661\n",
      "747-th epoch val loss 1.238494958194928\n",
      "748-th epoch train loss 1.1727113624572263\n",
      "748-th epoch val loss 1.2384952096870336\n",
      "749-th epoch train loss 1.17271164281265\n",
      "749-th epoch val loss 1.2384954572359974\n",
      "750-th epoch train loss 1.1727119187719108\n",
      "750-th epoch val loss 1.2384957009035913\n",
      "751-th epoch train loss 1.172712190403893\n",
      "751-th epoch val loss 1.2384959407506209\n",
      "752-th epoch train loss 1.172712457776402\n",
      "752-th epoch val loss 1.2384961768369418\n",
      "753-th epoch train loss 1.1727127209561834\n",
      "753-th epoch val loss 1.2384964092214732\n",
      "754-th epoch train loss 1.172712980008938\n",
      "754-th epoch val loss 1.2384966379622122\n",
      "755-th epoch train loss 1.172713234999339\n",
      "755-th epoch val loss 1.2384968631162496\n",
      "756-th epoch train loss 1.1727134859910462\n",
      "756-th epoch val loss 1.2384970847397814\n",
      "757-th epoch train loss 1.172713733046724\n",
      "757-th epoch val loss 1.2384973028881259\n",
      "758-th epoch train loss 1.172713976228057\n",
      "758-th epoch val loss 1.2384975176157356\n",
      "759-th epoch train loss 1.172714215595762\n",
      "759-th epoch val loss 1.2384977289762107\n",
      "760-th epoch train loss 1.1727144512096082\n",
      "760-th epoch val loss 1.2384979370223137\n",
      "761-th epoch train loss 1.172714683128427\n",
      "761-th epoch val loss 1.23849814180598\n",
      "762-th epoch train loss 1.172714911410131\n",
      "762-th epoch val loss 1.2384983433783334\n",
      "763-th epoch train loss 1.1727151361117245\n",
      "763-th epoch val loss 1.2384985417896979\n",
      "764-th epoch train loss 1.1727153572893196\n",
      "764-th epoch val loss 1.2384987370896092\n",
      "765-th epoch train loss 1.1727155749981515\n",
      "765-th epoch val loss 1.238498929326828\n",
      "766-th epoch train loss 1.1727157892925901\n",
      "766-th epoch val loss 1.2384991185493521\n",
      "767-th epoch train loss 1.1727160002261516\n",
      "767-th epoch val loss 1.2384993048044275\n",
      "768-th epoch train loss 1.1727162078515188\n",
      "768-th epoch val loss 1.2384994881385618\n",
      "769-th epoch train loss 1.1727164122205456\n",
      "769-th epoch val loss 1.2384996685975338\n",
      "770-th epoch train loss 1.1727166133842764\n",
      "770-th epoch val loss 1.2384998462264054\n",
      "771-th epoch train loss 1.1727168113929556\n",
      "771-th epoch val loss 1.2385000210695343\n",
      "772-th epoch train loss 1.1727170062960406\n",
      "772-th epoch val loss 1.2385001931705824\n",
      "773-th epoch train loss 1.1727171981422144\n",
      "773-th epoch val loss 1.2385003625725288\n",
      "774-th epoch train loss 1.172717386979398\n",
      "774-th epoch val loss 1.2385005293176807\n",
      "775-th epoch train loss 1.1727175728547625\n",
      "775-th epoch val loss 1.2385006934476812\n",
      "776-th epoch train loss 1.1727177558147384\n",
      "776-th epoch val loss 1.2385008550035221\n",
      "777-th epoch train loss 1.1727179359050306\n",
      "777-th epoch val loss 1.2385010140255532\n",
      "778-th epoch train loss 1.1727181131706277\n",
      "778-th epoch val loss 1.2385011705534934\n",
      "779-th epoch train loss 1.1727182876558135\n",
      "779-th epoch val loss 1.2385013246264387\n",
      "780-th epoch train loss 1.1727184594041784\n",
      "780-th epoch val loss 1.2385014762828728\n",
      "781-th epoch train loss 1.172718628458631\n",
      "781-th epoch val loss 1.2385016255606778\n",
      "782-th epoch train loss 1.1727187948614062\n",
      "782-th epoch val loss 1.2385017724971414\n",
      "783-th epoch train loss 1.1727189586540776\n",
      "783-th epoch val loss 1.2385019171289686\n",
      "784-th epoch train loss 1.1727191198775684\n",
      "784-th epoch val loss 1.238502059492288\n",
      "785-th epoch train loss 1.17271927857216\n",
      "785-th epoch val loss 1.2385021996226637\n",
      "786-th epoch train loss 1.1727194347775036\n",
      "786-th epoch val loss 1.2385023375551019\n",
      "787-th epoch train loss 1.172719588532628\n",
      "787-th epoch val loss 1.2385024733240617\n",
      "788-th epoch train loss 1.1727197398759515\n",
      "788-th epoch val loss 1.2385026069634606\n",
      "789-th epoch train loss 1.1727198888452908\n",
      "789-th epoch val loss 1.2385027385066871\n",
      "790-th epoch train loss 1.1727200354778693\n",
      "790-th epoch val loss 1.238502867986605\n",
      "791-th epoch train loss 1.1727201798103266\n",
      "791-th epoch val loss 1.2385029954355633\n",
      "792-th epoch train loss 1.1727203218787305\n",
      "792-th epoch val loss 1.2385031208854058\n",
      "793-th epoch train loss 1.1727204617185816\n",
      "793-th epoch val loss 1.2385032443674757\n",
      "794-th epoch train loss 1.172720599364825\n",
      "794-th epoch val loss 1.2385033659126268\n",
      "795-th epoch train loss 1.1727207348518587\n",
      "795-th epoch val loss 1.2385034855512274\n",
      "796-th epoch train loss 1.1727208682135402\n",
      "796-th epoch val loss 1.2385036033131729\n",
      "797-th epoch train loss 1.1727209994831989\n",
      "797-th epoch val loss 1.2385037192278878\n",
      "798-th epoch train loss 1.1727211286936396\n",
      "798-th epoch val loss 1.2385038333243372\n",
      "799-th epoch train loss 1.172721255877154\n",
      "799-th epoch val loss 1.2385039456310307\n",
      "800-th epoch train loss 1.1727213810655281\n",
      "800-th epoch val loss 1.2385040561760334\n",
      "801-th epoch train loss 1.172721504290049\n",
      "801-th epoch val loss 1.2385041649869688\n",
      "802-th epoch train loss 1.1727216255815145\n",
      "802-th epoch val loss 1.2385042720910289\n",
      "803-th epoch train loss 1.1727217449702385\n",
      "803-th epoch val loss 1.2385043775149789\n",
      "804-th epoch train loss 1.1727218624860607\n",
      "804-th epoch val loss 1.2385044812851644\n",
      "805-th epoch train loss 1.1727219781583529\n",
      "805-th epoch val loss 1.2385045834275188\n",
      "806-th epoch train loss 1.1727220920160266\n",
      "806-th epoch val loss 1.2385046839675686\n",
      "807-th epoch train loss 1.1727222040875398\n",
      "807-th epoch val loss 1.238504782930441\n",
      "808-th epoch train loss 1.1727223144009051\n",
      "808-th epoch val loss 1.2385048803408685\n",
      "809-th epoch train loss 1.1727224229836954\n",
      "809-th epoch val loss 1.238504976223196\n",
      "810-th epoch train loss 1.1727225298630526\n",
      "810-th epoch val loss 1.2385050706013885\n",
      "811-th epoch train loss 1.1727226350656912\n",
      "811-th epoch val loss 1.2385051634990332\n",
      "812-th epoch train loss 1.172722738617909\n",
      "812-th epoch val loss 1.2385052549393494\n",
      "813-th epoch train loss 1.1727228405455903\n",
      "813-th epoch val loss 1.2385053449451917\n",
      "814-th epoch train loss 1.1727229408742135\n",
      "814-th epoch val loss 1.2385054335390553\n",
      "815-th epoch train loss 1.1727230396288593\n",
      "815-th epoch val loss 1.238505520743086\n",
      "816-th epoch train loss 1.172723136834213\n",
      "816-th epoch val loss 1.2385056065790796\n",
      "817-th epoch train loss 1.1727232325145747\n",
      "817-th epoch val loss 1.2385056910684915\n",
      "818-th epoch train loss 1.172723326693863\n",
      "818-th epoch val loss 1.2385057742324406\n",
      "819-th epoch train loss 1.1727234193956206\n",
      "819-th epoch val loss 1.2385058560917144\n",
      "820-th epoch train loss 1.172723510643023\n",
      "820-th epoch val loss 1.2385059366667752\n",
      "821-th epoch train loss 1.1727236004588808\n",
      "821-th epoch val loss 1.2385060159777648\n",
      "822-th epoch train loss 1.172723688865648\n",
      "822-th epoch val loss 1.2385060940445078\n",
      "823-th epoch train loss 1.1727237758854259\n",
      "823-th epoch val loss 1.2385061708865202\n",
      "824-th epoch train loss 1.1727238615399689\n",
      "824-th epoch val loss 1.2385062465230094\n",
      "825-th epoch train loss 1.1727239458506915\n",
      "825-th epoch val loss 1.2385063209728844\n",
      "826-th epoch train loss 1.1727240288386724\n",
      "826-th epoch val loss 1.2385063942547558\n",
      "827-th epoch train loss 1.1727241105246586\n",
      "827-th epoch val loss 1.2385064663869432\n",
      "828-th epoch train loss 1.1727241909290729\n",
      "828-th epoch val loss 1.2385065373874795\n",
      "829-th epoch train loss 1.1727242700720173\n",
      "829-th epoch val loss 1.2385066072741142\n",
      "830-th epoch train loss 1.1727243479732798\n",
      "830-th epoch val loss 1.2385066760643186\n",
      "831-th epoch train loss 1.1727244246523365\n",
      "831-th epoch val loss 1.2385067437752908\n",
      "832-th epoch train loss 1.1727245001283588\n",
      "832-th epoch val loss 1.2385068104239576\n",
      "833-th epoch train loss 1.1727245744202175\n",
      "833-th epoch val loss 1.2385068760269824\n",
      "834-th epoch train loss 1.1727246475464876\n",
      "834-th epoch val loss 1.2385069406007663\n",
      "835-th epoch train loss 1.1727247195254538\n",
      "835-th epoch val loss 1.2385070041614539\n",
      "836-th epoch train loss 1.172724790375112\n",
      "836-th epoch val loss 1.2385070667249352\n",
      "837-th epoch train loss 1.172724860113177\n",
      "837-th epoch val loss 1.2385071283068525\n",
      "838-th epoch train loss 1.1727249287570864\n",
      "838-th epoch val loss 1.2385071889226027\n",
      "839-th epoch train loss 1.1727249963240038\n",
      "839-th epoch val loss 1.2385072485873407\n",
      "840-th epoch train loss 1.1727250628308232\n",
      "840-th epoch val loss 1.238507307315984\n",
      "841-th epoch train loss 1.1727251282941744\n",
      "841-th epoch val loss 1.238507365123217\n",
      "842-th epoch train loss 1.1727251927304265\n",
      "842-th epoch val loss 1.2385074220234922\n",
      "843-th epoch train loss 1.1727252561556913\n",
      "843-th epoch val loss 1.238507478031037\n",
      "844-th epoch train loss 1.1727253185858286\n",
      "844-th epoch val loss 1.2385075331598552\n",
      "845-th epoch train loss 1.172725380036448\n",
      "845-th epoch val loss 1.2385075874237301\n",
      "846-th epoch train loss 1.1727254405229173\n",
      "846-th epoch val loss 1.238507640836231\n",
      "847-th epoch train loss 1.1727255000603594\n",
      "847-th epoch val loss 1.2385076934107118\n",
      "848-th epoch train loss 1.1727255586636633\n",
      "848-th epoch val loss 1.238507745160319\n",
      "849-th epoch train loss 1.1727256163474833\n",
      "849-th epoch val loss 1.2385077960979924\n",
      "850-th epoch train loss 1.1727256731262428\n",
      "850-th epoch val loss 1.2385078462364685\n",
      "851-th epoch train loss 1.172725729014141\n",
      "851-th epoch val loss 1.2385078955882842\n",
      "852-th epoch train loss 1.1727257840251537\n",
      "852-th epoch val loss 1.2385079441657807\n",
      "853-th epoch train loss 1.1727258381730365\n",
      "853-th epoch val loss 1.2385079919811042\n",
      "854-th epoch train loss 1.1727258914713306\n",
      "854-th epoch val loss 1.238508039046211\n",
      "855-th epoch train loss 1.1727259439333637\n",
      "855-th epoch val loss 1.2385080853728705\n",
      "856-th epoch train loss 1.1727259955722558\n",
      "856-th epoch val loss 1.2385081309726669\n",
      "857-th epoch train loss 1.1727260464009204\n",
      "857-th epoch val loss 1.238508175857003\n",
      "858-th epoch train loss 1.1727260964320676\n",
      "858-th epoch val loss 1.238508220037102\n",
      "859-th epoch train loss 1.1727261456782097\n",
      "859-th epoch val loss 1.2385082635240126\n",
      "860-th epoch train loss 1.172726194151662\n",
      "860-th epoch val loss 1.238508306328609\n",
      "861-th epoch train loss 1.1727262418645468\n",
      "861-th epoch val loss 1.2385083484615953\n",
      "862-th epoch train loss 1.1727262888287955\n",
      "862-th epoch val loss 1.2385083899335072\n",
      "863-th epoch train loss 1.1727263350561536\n",
      "863-th epoch val loss 1.238508430754716\n",
      "864-th epoch train loss 1.172726380558182\n",
      "864-th epoch val loss 1.2385084709354302\n",
      "865-th epoch train loss 1.1727264253462595\n",
      "865-th epoch val loss 1.2385085104856972\n",
      "866-th epoch train loss 1.1727264694315882\n",
      "866-th epoch val loss 1.2385085494154084\n",
      "867-th epoch train loss 1.172726512825192\n",
      "867-th epoch val loss 1.2385085877342992\n",
      "868-th epoch train loss 1.172726555537924\n",
      "868-th epoch val loss 1.238508625451952\n",
      "869-th epoch train loss 1.172726597580466\n",
      "869-th epoch val loss 1.2385086625777986\n",
      "870-th epoch train loss 1.1727266389633324\n",
      "870-th epoch val loss 1.2385086991211247\n",
      "871-th epoch train loss 1.1727266796968727\n",
      "871-th epoch val loss 1.2385087350910682\n",
      "872-th epoch train loss 1.172726719791275\n",
      "872-th epoch val loss 1.2385087704966256\n",
      "873-th epoch train loss 1.1727267592565658\n",
      "873-th epoch val loss 1.2385088053466506\n",
      "874-th epoch train loss 1.1727267981026155\n",
      "874-th epoch val loss 1.2385088396498587\n",
      "875-th epoch train loss 1.1727268363391397\n",
      "875-th epoch val loss 1.238508873414829\n",
      "876-th epoch train loss 1.172726873975701\n",
      "876-th epoch val loss 1.238508906650006\n",
      "877-th epoch train loss 1.1727269110217136\n",
      "877-th epoch val loss 1.2385089393637017\n",
      "878-th epoch train loss 1.172726947486441\n",
      "878-th epoch val loss 1.2385089715640973\n",
      "879-th epoch train loss 1.1727269833790042\n",
      "879-th epoch val loss 1.2385090032592454\n",
      "880-th epoch train loss 1.17272701870838\n",
      "880-th epoch val loss 1.2385090344570737\n",
      "881-th epoch train loss 1.172727053483405\n",
      "881-th epoch val loss 1.2385090651653843\n",
      "882-th epoch train loss 1.1727270877127758\n",
      "882-th epoch val loss 1.2385090953918576\n",
      "883-th epoch train loss 1.1727271214050536\n",
      "883-th epoch val loss 1.2385091251440525\n",
      "884-th epoch train loss 1.172727154568666\n",
      "884-th epoch val loss 1.238509154429411\n",
      "885-th epoch train loss 1.1727271872119063\n",
      "885-th epoch val loss 1.2385091832552564\n",
      "886-th epoch train loss 1.1727272193429397\n",
      "886-th epoch val loss 1.238509211628799\n",
      "887-th epoch train loss 1.1727272509698023\n",
      "887-th epoch val loss 1.2385092395571353\n",
      "888-th epoch train loss 1.1727272821004047\n",
      "888-th epoch val loss 1.2385092670472488\n",
      "889-th epoch train loss 1.172727312742533\n",
      "889-th epoch val loss 1.238509294106017\n",
      "890-th epoch train loss 1.1727273429038512\n",
      "890-th epoch val loss 1.2385093207402056\n",
      "891-th epoch train loss 1.1727273725919034\n",
      "891-th epoch val loss 1.2385093469564776\n",
      "892-th epoch train loss 1.1727274018141152\n",
      "892-th epoch val loss 1.2385093727613896\n",
      "893-th epoch train loss 1.1727274305777948\n",
      "893-th epoch val loss 1.2385093981613953\n",
      "894-th epoch train loss 1.1727274588901375\n",
      "894-th epoch val loss 1.238509423162848\n",
      "895-th epoch train loss 1.172727486758224\n",
      "895-th epoch val loss 1.2385094477719998\n",
      "896-th epoch train loss 1.1727275141890259\n",
      "896-th epoch val loss 1.2385094719950072\n",
      "897-th epoch train loss 1.172727541189403\n",
      "897-th epoch val loss 1.2385094958379284\n",
      "898-th epoch train loss 1.172727567766109\n",
      "898-th epoch val loss 1.2385095193067261\n",
      "899-th epoch train loss 1.172727593925792\n",
      "899-th epoch val loss 1.238509542407271\n",
      "900-th epoch train loss 1.1727276196749943\n",
      "900-th epoch val loss 1.238509565145341\n",
      "901-th epoch train loss 1.1727276450201574\n",
      "901-th epoch val loss 1.2385095875266225\n",
      "902-th epoch train loss 1.1727276699676201\n",
      "902-th epoch val loss 1.238509609556715\n",
      "903-th epoch train loss 1.1727276945236227\n",
      "903-th epoch val loss 1.2385096312411275\n",
      "904-th epoch train loss 1.172727718694307\n",
      "904-th epoch val loss 1.2385096525852837\n",
      "905-th epoch train loss 1.1727277424857199\n",
      "905-th epoch val loss 1.2385096735945231\n",
      "906-th epoch train loss 1.1727277659038111\n",
      "906-th epoch val loss 1.2385096942740998\n",
      "907-th epoch train loss 1.172727788954438\n",
      "907-th epoch val loss 1.238509714629187\n",
      "908-th epoch train loss 1.1727278116433675\n",
      "908-th epoch val loss 1.2385097346648752\n",
      "909-th epoch train loss 1.1727278339762741\n",
      "909-th epoch val loss 1.2385097543861763\n",
      "910-th epoch train loss 1.1727278559587444\n",
      "910-th epoch val loss 1.2385097737980233\n",
      "911-th epoch train loss 1.172727877596276\n",
      "911-th epoch val loss 1.238509792905271\n",
      "912-th epoch train loss 1.1727278988942822\n",
      "912-th epoch val loss 1.2385098117126996\n",
      "913-th epoch train loss 1.1727279198580902\n",
      "913-th epoch val loss 1.2385098302250133\n",
      "914-th epoch train loss 1.1727279404929436\n",
      "914-th epoch val loss 1.2385098484468415\n",
      "915-th epoch train loss 1.172727960804004\n",
      "915-th epoch val loss 1.238509866382743\n",
      "916-th epoch train loss 1.1727279807963518\n",
      "916-th epoch val loss 1.238509884037204\n",
      "917-th epoch train loss 1.1727280004749874\n",
      "917-th epoch val loss 1.2385099014146397\n",
      "918-th epoch train loss 1.1727280198448342\n",
      "918-th epoch val loss 1.2385099185193982\n",
      "919-th epoch train loss 1.1727280389107368\n",
      "919-th epoch val loss 1.238509935355757\n",
      "920-th epoch train loss 1.1727280576774641\n",
      "920-th epoch val loss 1.238509951927927\n",
      "921-th epoch train loss 1.1727280761497108\n",
      "921-th epoch val loss 1.2385099682400547\n",
      "922-th epoch train loss 1.1727280943320975\n",
      "922-th epoch val loss 1.238509984296219\n",
      "923-th epoch train loss 1.1727281122291722\n",
      "923-th epoch val loss 1.2385100001004379\n",
      "924-th epoch train loss 1.172728129845412\n",
      "924-th epoch val loss 1.238510015656663\n",
      "925-th epoch train loss 1.172728147185223\n",
      "925-th epoch val loss 1.2385100309687862\n",
      "926-th epoch train loss 1.172728164252943\n",
      "926-th epoch val loss 1.2385100460406377\n",
      "927-th epoch train loss 1.1727281810528412\n",
      "927-th epoch val loss 1.2385100608759871\n",
      "928-th epoch train loss 1.1727281975891204\n",
      "928-th epoch val loss 1.2385100754785463\n",
      "929-th epoch train loss 1.1727282138659165\n",
      "929-th epoch val loss 1.238510089851967\n",
      "930-th epoch train loss 1.172728229887301\n",
      "930-th epoch val loss 1.2385101039998443\n",
      "931-th epoch train loss 1.172728245657282\n",
      "931-th epoch val loss 1.238510117925718\n",
      "932-th epoch train loss 1.1727282611798044\n",
      "932-th epoch val loss 1.238510131633072\n",
      "933-th epoch train loss 1.172728276458751\n",
      "933-th epoch val loss 1.238510145125334\n",
      "934-th epoch train loss 1.1727282914979433\n",
      "934-th epoch val loss 1.238510158405879\n",
      "935-th epoch train loss 1.172728306301144\n",
      "935-th epoch val loss 1.2385101714780302\n",
      "936-th epoch train loss 1.1727283208720558\n",
      "936-th epoch val loss 1.238510184345057\n",
      "937-th epoch train loss 1.1727283352143232\n",
      "937-th epoch val loss 1.2385101970101766\n",
      "938-th epoch train loss 1.1727283493315341\n",
      "938-th epoch val loss 1.238510209476559\n",
      "939-th epoch train loss 1.1727283632272203\n",
      "939-th epoch val loss 1.2385102217473216\n",
      "940-th epoch train loss 1.1727283769048575\n",
      "940-th epoch val loss 1.2385102338255343\n",
      "941-th epoch train loss 1.1727283903678671\n",
      "941-th epoch val loss 1.238510245714218\n",
      "942-th epoch train loss 1.172728403619617\n",
      "942-th epoch val loss 1.2385102574163471\n",
      "943-th epoch train loss 1.1727284166634226\n",
      "943-th epoch val loss 1.2385102689348497\n",
      "944-th epoch train loss 1.172728429502546\n",
      "944-th epoch val loss 1.2385102802726051\n",
      "945-th epoch train loss 1.1727284421401996\n",
      "945-th epoch val loss 1.2385102914324508\n",
      "946-th epoch train loss 1.1727284545795442\n",
      "946-th epoch val loss 1.2385103024171784\n",
      "947-th epoch train loss 1.172728466823692\n",
      "947-th epoch val loss 1.238510313229535\n",
      "948-th epoch train loss 1.172728478875706\n",
      "948-th epoch val loss 1.2385103238722261\n",
      "949-th epoch train loss 1.1727284907386\n",
      "949-th epoch val loss 1.2385103343479134\n",
      "950-th epoch train loss 1.1727285024153427\n",
      "950-th epoch val loss 1.2385103446592174\n",
      "951-th epoch train loss 1.1727285139088544\n",
      "951-th epoch val loss 1.2385103548087177\n",
      "952-th epoch train loss 1.1727285252220105\n",
      "952-th epoch val loss 1.2385103647989533\n",
      "953-th epoch train loss 1.1727285363576414\n",
      "953-th epoch val loss 1.2385103746324235\n",
      "954-th epoch train loss 1.1727285473185327\n",
      "954-th epoch val loss 1.2385103843115874\n",
      "955-th epoch train loss 1.1727285581074254\n",
      "955-th epoch val loss 1.238510393838867\n",
      "956-th epoch train loss 1.1727285687270197\n",
      "956-th epoch val loss 1.2385104032166454\n",
      "957-th epoch train loss 1.1727285791799715\n",
      "957-th epoch val loss 1.2385104124472683\n",
      "958-th epoch train loss 1.1727285894688955\n",
      "958-th epoch val loss 1.238510421533045\n",
      "959-th epoch train loss 1.1727285995963654\n",
      "959-th epoch val loss 1.238510430476248\n",
      "960-th epoch train loss 1.1727286095649156\n",
      "960-th epoch val loss 1.2385104392791149\n",
      "961-th epoch train loss 1.1727286193770392\n",
      "961-th epoch val loss 1.238510447943848\n",
      "962-th epoch train loss 1.1727286290351908\n",
      "962-th epoch val loss 1.2385104564726142\n",
      "963-th epoch train loss 1.1727286385417863\n",
      "963-th epoch val loss 1.2385104648675467\n",
      "964-th epoch train loss 1.1727286478992038\n",
      "964-th epoch val loss 1.2385104731307464\n",
      "965-th epoch train loss 1.1727286571097848\n",
      "965-th epoch val loss 1.23851048126428\n",
      "966-th epoch train loss 1.1727286661758325\n",
      "966-th epoch val loss 1.2385104892701826\n",
      "967-th epoch train loss 1.1727286750996158\n",
      "967-th epoch val loss 1.238510497150456\n",
      "968-th epoch train loss 1.1727286838833668\n",
      "968-th epoch val loss 1.2385105049070728\n",
      "969-th epoch train loss 1.1727286925292821\n",
      "969-th epoch val loss 1.2385105125419722\n",
      "970-th epoch train loss 1.1727287010395255\n",
      "970-th epoch val loss 1.2385105200570645\n",
      "971-th epoch train loss 1.1727287094162255\n",
      "971-th epoch val loss 1.2385105274542305\n",
      "972-th epoch train loss 1.172728717661478\n",
      "972-th epoch val loss 1.2385105347353198\n",
      "973-th epoch train loss 1.1727287257773453\n",
      "973-th epoch val loss 1.238510541902154\n",
      "974-th epoch train loss 1.1727287337658574\n",
      "974-th epoch val loss 1.2385105489565256\n",
      "975-th epoch train loss 1.1727287416290129\n",
      "975-th epoch val loss 1.2385105559001999\n",
      "976-th epoch train loss 1.1727287493687797\n",
      "976-th epoch val loss 1.2385105627349133\n",
      "977-th epoch train loss 1.1727287569870926\n",
      "977-th epoch val loss 1.2385105694623761\n",
      "978-th epoch train loss 1.1727287644858586\n",
      "978-th epoch val loss 1.2385105760842714\n",
      "979-th epoch train loss 1.1727287718669526\n",
      "979-th epoch val loss 1.2385105826022549\n",
      "980-th epoch train loss 1.172728779132222\n",
      "980-th epoch val loss 1.2385105890179582\n",
      "981-th epoch train loss 1.1727287862834839\n",
      "981-th epoch val loss 1.2385105953329856\n",
      "982-th epoch train loss 1.1727287933225274\n",
      "982-th epoch val loss 1.2385106015489173\n",
      "983-th epoch train loss 1.1727288002511134\n",
      "983-th epoch val loss 1.2385106076673078\n",
      "984-th epoch train loss 1.1727288070709747\n",
      "984-th epoch val loss 1.2385106136896877\n",
      "985-th epoch train loss 1.1727288137838177\n",
      "985-th epoch val loss 1.2385106196175641\n",
      "986-th epoch train loss 1.1727288203913222\n",
      "986-th epoch val loss 1.2385106254524194\n",
      "987-th epoch train loss 1.1727288268951406\n",
      "987-th epoch val loss 1.2385106311957135\n",
      "988-th epoch train loss 1.1727288332969\n",
      "988-th epoch val loss 1.2385106368488832\n",
      "989-th epoch train loss 1.1727288395982018\n",
      "989-th epoch val loss 1.2385106424133425\n",
      "990-th epoch train loss 1.172728845800623\n",
      "990-th epoch val loss 1.238510647890484\n",
      "991-th epoch train loss 1.1727288519057144\n",
      "991-th epoch val loss 1.2385106532816772\n",
      "992-th epoch train loss 1.1727288579150041\n",
      "992-th epoch val loss 1.2385106585882708\n",
      "993-th epoch train loss 1.172728863829995\n",
      "993-th epoch val loss 1.2385106638115932\n",
      "994-th epoch train loss 1.1727288696521667\n",
      "994-th epoch val loss 1.2385106689529501\n",
      "995-th epoch train loss 1.1727288753829752\n",
      "995-th epoch val loss 1.238510674013628\n",
      "996-th epoch train loss 1.1727288810238554\n",
      "996-th epoch val loss 1.2385106789948932\n",
      "997-th epoch train loss 1.172728886576218\n",
      "997-th epoch val loss 1.2385106838979913\n",
      "998-th epoch train loss 1.1727288920414516\n",
      "998-th epoch val loss 1.2385106887241495\n",
      "999-th epoch train loss 1.1727288974209236\n",
      "999-th epoch val loss 1.2385106934745749\n"
     ]
    }
   ],
   "source": [
    "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
    "slr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adb359c3-76b3-47f4-9ce8-390f21f90438",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = slr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "183c015e-474f-4004-8a1f-89805218e48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95051633330.28659"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(np.exp(pred_test), np.exp(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a2075f-7185-419f-9505-ef2dd468aec1",
   "metadata": {},
   "source": [
    "## Problem 7 Plotting the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb11b63c-b862-45d1-a380-15fc69e618f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x73174a050940>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(slr.loss)\n",
    "plt.plot(slr.val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
